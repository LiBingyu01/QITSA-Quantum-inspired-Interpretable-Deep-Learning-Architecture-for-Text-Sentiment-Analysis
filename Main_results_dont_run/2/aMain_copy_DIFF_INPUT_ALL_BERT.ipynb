{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, Embedding_dim, num_layers):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "\n",
    "        # GRU layer\n",
    "        self.rnn = nn.GRU(Embedding_dim, Embedding_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "        # self.scale_layer = nn.Linear(2 * Embedding_dim, Embedding_dim)\n",
    "\n",
    "        # Linear layer for skip connection\n",
    "        self.middle_layer = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through GRU layer\n",
    "        output, _ = self.rnn(x)\n",
    "\n",
    "        # Skip connection: Add output of middle layer to GRU layer output\n",
    "        output = output + self.middle_layer(x)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "# -------------------- 01 Model_best_copy8672_cnn_maxpooling ------------------------\n",
    "print(\"Model_best_copy8672_cnn_maxpooling\")\n",
    "class run_complex_network(nn.Module):\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, output_dim, num_layers, pad_idx, VOCAB_SIZE, batch):\n",
    "        super().__init__()\n",
    "\n",
    "        # ================ 1、词嵌入模块 ================\n",
    "        # ------ 实部，预训练模型 -------\n",
    "        self.amplitude_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "\n",
    "        # self.amplitude_embedding2 = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ------ 虚部，传统模型 -------\n",
    "        self.phase_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "        # self.phase_embedding = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "        # self.phase_embedding_sentiment = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ================ 2、GRU模块 信息提取 ================\n",
    "        # 输出张量的维度(L:sequence_length,N:batch_size,hidden_size)\n",
    "        self.gru1 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "        self.gru2 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "\n",
    "        self.self_attention1 = self_attention(Embedding_dim=embedding_dim)\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "\n",
    "        self.projection_Euler = projection_Euler(Embedding=embedding_dim, num_layers=num_layers)\n",
    "        self.projection_CalculateMatrixattension = projection_CalculateMatrixQ(Embedding_dim=embedding_dim)\n",
    "        self.projection_measurement = projection_Measurement(embedding_dim)\n",
    "\n",
    "        # ================ 4、卷积 ================\n",
    "        self.Conv2dOne = nn.Conv2d(1, 1, 3)  # 实部\n",
    "        self.Conv2dTwo = nn.Conv2d(1, 1, 3)  # 虚部\n",
    "\n",
    "        # ================ 5、池化 ================\n",
    "\n",
    "        self.MaxPool1 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 实部\n",
    "        self.MaxPool2 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 虚部\n",
    "\n",
    "        # ================ 6、全连接层 ================\n",
    "        self.fc1 = nn.Linear(2 * (embedding_dim - 2), 10)\n",
    "        self.fc2 = nn.Linear(10, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # ================ 1、文本嵌入到向量 ================\n",
    "        amplitude_is_WordEmbedding = self.amplitude_embedding(text)\n",
    "\n",
    "        phase_is_sentiment = self.phase_embedding(text)\n",
    "\n",
    "        # ================ 2、实部进行GRU处理 ================\n",
    "        amplitude_plus = self.gru1(amplitude_is_WordEmbedding)\n",
    "        amplitude_plus2 = self.self_attention1(amplitude_plus)\n",
    "        amplitude_plus2 = amplitude_plus2.permute(1, 0, 2)\n",
    "\n",
    "        phase_plus = self.gru2(phase_is_sentiment)\n",
    "\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "        embedded = [amplitude_plus2, phase_plus]\n",
    "\n",
    "        Euler_realAimag = self.projection_Euler(embedded)\n",
    "        Matrix_realAimag = self.projection_CalculateMatrixattension(Euler_realAimag)\n",
    "        Project_realAimag = self.projection_measurement(Matrix_realAimag)\n",
    "\n",
    "        MatrixReal = Project_realAimag[0]\n",
    "        MatrixImag = Project_realAimag[1]\n",
    "\n",
    "        # ================ 4、卷积操作 ================\n",
    "        # 5.1 --对于【实部】进行卷积\n",
    "        Conv_real = self.Conv2dOne(MatrixReal)\n",
    "        # 5.2 --对于【虚部】进行卷积\n",
    "        Conv_imag = self.Conv2dTwo(MatrixImag)\n",
    "\n",
    "        # ================ 5、池化操作 ================\n",
    "        # 6.1 --对于【实部】进行池化\n",
    "        Max_real = self.MaxPool1(torch.sigmoid(Conv_real))\n",
    "        # 6.2 --对于【虚部】进行池化\n",
    "        Max_imag = self.MaxPool2(torch.sigmoid(Conv_imag))\n",
    "\n",
    "        # ================ 6、全连接 ================\n",
    "        fc1 = self.fc1(torch.cat((Max_real, Max_imag), dim=3))\n",
    "        fc2 = torch.sigmoid(self.fc2(torch.sigmoid(fc1)))\n",
    "\n",
    "        return fc2  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_best_copy8672_cnn_maxpooling\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def multiply(input1, input2):\n",
    "    return torch.sum(torch.mul(input1, input2)).item()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    # print('batch: 模型预测值', rounded_preds,'真实标签', y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def calculation_recall(preds, y):\n",
    "    pre = preds.detach()\n",
    "    pre_y = y.detach()\n",
    "    rounded_preds = torch.round(pre)\n",
    "    \n",
    "    TP = (torch.from_numpy(rounded_preds.cpu().numpy().astype(int)) & torch.from_numpy(pre_y.cpu().numpy().astype(int))).sum()\n",
    "    recall = TP / (y.cpu().sum())\n",
    "    return recall\n",
    "\n",
    "def f1_score_avg(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    f1_score(y, rounded_preds, 'macro')\n",
    "\n",
    "# 定义训练流程\n",
    "# 把训练数据投入模型进行训练\n",
    "def train(model, iterator, optimizer, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        text = batch[0].cuda(CUDA_NUMBER)\n",
    "        label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text).squeeze(1)\n",
    "        # print(predictions)\n",
    "        loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "        acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "        recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "        label_score = label.tolist()\n",
    "\n",
    "        y_true = label_score.copy()\n",
    "\n",
    "        predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "\n",
    "        y_pred = predictions_score.copy()\n",
    "\n",
    "        F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        epoch_recall += recall.item()\n",
    "\n",
    "        epoch_f1_score += F1Score.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "# 评价\n",
    "def evaluate(model, iterator, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch[0].cuda(CUDA_NUMBER)\n",
    "            label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "            predictions = model(text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "            acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "            recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "            label_score = label.tolist()\n",
    "            y_true = label_score.copy()\n",
    "            predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "            y_pred = predictions_score.copy()\n",
    "            F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1_score += F1Score.item()\n",
    "            epoch_recall += recall.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "num_layers = 2\n",
    "print(\"num_layers\",num_layers)\n",
    "# vocab_size = 19192, embedding_dim = 100, hidden_dim = 3, output_dim = 1, dropout = 0.5, pad_idx = 1\n",
    "PAD_IDX = 1\n",
    "print('pad_idx', PAD_IDX)\n",
    "\n",
    "model = run_complex_network(word_vector_weight_matrix,\n",
    "                            EMBEDDING_DIM,\n",
    "                            HIDDEN_DIM,\n",
    "                            OUTPUT_DIM,\n",
    "                            num_layers,\n",
    "                            PAD_IDX,\n",
    "                            VOCAB_SIZE,\n",
    "                            BATCH_SIZE)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())#优化器使用的是Adam\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "best_test_acc = float('-inf')\n",
    "print(0.5 > best_test_acc)\n",
    "\n",
    "best_acc = -1\n",
    "best_f1_score = -1\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "f1_score_test = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc, train_f1_score, train_recall \\\n",
    "        = train(model, train_iterator, optimizer, criterion, CUDA_NUMBER)\n",
    "\n",
    "    test_loss, test_acc, test_f1_score, test_recall \\\n",
    "        = evaluate(model, test_iterator, criterion, CUDA_NUMBER)\n",
    "\n",
    "    acc_train.append(train_acc)\n",
    "    acc_test.append(test_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    loss_test.append(test_loss)\n",
    "    f1_score_test.append(test_f1_score)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'SST_GRU_Conv_model.pt')\n",
    "\n",
    "    # print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    # print(\n",
    "    #     f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}% | Train f1_score: {train_f1_score * 100:.2f}% | Train recall: {train_recall * 100:.2f}%')\n",
    "    # print(\n",
    "    #     f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc * 100:.2f}% | Test f1_score: {test_f1_score * 100:.2f}%| best_acc: {best_acc * 100:.2f}% | Test recall: {test_recall * 100:.2f}%')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t test  Loss: {test_loss:.3f} | test  Acc: {test_acc * 100:.2f}%')\n",
    "    print(f'\\t best  test acc: {best_test_acc * 100:.2f}%')\n",
    "\n",
    "with open('./train/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_train:\n",
    "        f.write(str(i))\n",
    "    f.close()\n",
    "\n",
    "with open('./test/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/F1_Score/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in f1_score_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/acc/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in acc_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/recall/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in test_path_ls:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "#\n",
    "colors1 = '#00CED1' \n",
    "colors2 = '#DC143C'\n",
    "colors3 = '#FFA500'\n",
    "colors4 = '#0000aFF'\n",
    "colors5 = '#FF0000'\n",
    "\n",
    "# 画初始点\n",
    "plt.figure(1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(DATA_SET + ' Loss')\n",
    "plt.plot(range(N_EPOCHS), acc_train[0:N_EPOCHS], 'r-o', label='train_loss')\n",
    "plt.plot(range(N_EPOCHS), acc_test[0:N_EPOCHS], 'c-*', label='test_loss')\n",
    "plt.xlim(-0.01, N_EPOCHS)\n",
    "plt.ylim(-0.01, 1)\n",
    "# plt.plot(range(2), sp, c=colors3)\n",
    "plt.legend()\n",
    "plt.savefig('pic' + DATA_SET + '.pdf', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num_layers 2\n",
      "pad_idx 1\n",
      "The model has 3,604,201 trainable parameters\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.687 | Train Acc: 52.82%\n",
      "\t test  Loss: 0.656 | test  Acc: 65.03%\n",
      "\t best  test acc: 65.03%\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.546 | Train Acc: 74.33%\n",
      "\t test  Loss: 0.518 | test  Acc: 75.58%\n",
      "\t best  test acc: 75.58%\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.369 | Train Acc: 85.23%\n",
      "\t test  Loss: 0.562 | test  Acc: 75.86%\n",
      "\t best  test acc: 75.86%\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.230 | Train Acc: 92.26%\n",
      "\t test  Loss: 0.648 | test  Acc: 76.09%\n",
      "\t best  test acc: 76.09%\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.148 | Train Acc: 95.82%\n",
      "\t test  Loss: 0.682 | test  Acc: 76.74%\n",
      "\t best  test acc: 76.74%\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.110 | Train Acc: 97.31%\n",
      "\t test  Loss: 0.665 | test  Acc: 78.42%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 07 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.091 | Train Acc: 97.73%\n",
      "\t test  Loss: 0.753 | test  Acc: 77.76%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.071 | Train Acc: 98.38%\n",
      "\t test  Loss: 0.819 | test  Acc: 77.58%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 09 | Epoch Time: 0m 12s\n",
      "\t Train Loss: 0.059 | Train Acc: 98.64%\n",
      "\t test  Loss: 0.923 | test  Acc: 77.20%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.045 | Train Acc: 98.99%\n",
      "\t test  Loss: 1.064 | test  Acc: 75.47%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 11 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.042 | Train Acc: 99.03%\n",
      "\t test  Loss: 1.027 | test  Acc: 75.57%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 12 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.026 | Train Acc: 99.45%\n",
      "\t test  Loss: 1.143 | test  Acc: 75.43%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.027 | Train Acc: 99.37%\n",
      "\t test  Loss: 1.134 | test  Acc: 74.45%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 14 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.020 | Train Acc: 99.56%\n",
      "\t test  Loss: 1.259 | test  Acc: 75.25%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 15 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.011 | Train Acc: 99.78%\n",
      "\t test  Loss: 1.245 | test  Acc: 75.71%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 16 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.009 | Train Acc: 99.87%\n",
      "\t test  Loss: 1.309 | test  Acc: 75.85%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 17 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.020 | Train Acc: 99.56%\n",
      "\t test  Loss: 1.180 | test  Acc: 75.62%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 18 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.015 | Train Acc: 99.64%\n",
      "\t test  Loss: 1.277 | test  Acc: 74.64%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 19 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.006 | Train Acc: 99.88%\n",
      "\t test  Loss: 1.388 | test  Acc: 75.47%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 20 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.008 | Train Acc: 99.78%\n",
      "\t test  Loss: 1.368 | test  Acc: 76.45%\n",
      "\t best  test acc: 78.42%\n",
      "Epoch: 21 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.007 | Train Acc: 99.86%\n",
      "\t test  Loss: 1.355 | test  Acc: 77.20%\n",
      "\t best  test acc: 78.42%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "print(torch.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DATA_SET = 'MPQA'\n",
    "N_EPOCHS = 21\n",
    "\n",
    "print(\"DATA_SET:\", DATA_SET)\n",
    "CUDA_NUMBER = 0\n",
    "\n",
    "# 0-->MR ||  1-->CR ||  2-->SST  ||  3-->SUBJ ||  4-->MPQA\n",
    "dataset_dict = {}\n",
    "dataset_dict['MR'] = 0\n",
    "dataset_dict['CR'] = 1\n",
    "dataset_dict['SST'] = 2\n",
    "dataset_dict['SUBJ'] = 3\n",
    "dataset_dict['MPQA'] = 4\n",
    "\n",
    "# 设置要训练的数据集\n",
    "DATASET_NUMBER = dataset_dict[DATA_SET]\n",
    "\n",
    "train_path_ls = []\n",
    "test_path_ls = []\n",
    "word_vector_path_ls = []\n",
    "train_path_ls.append(\"./DataSet/MR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/CR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SST/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SUBJ/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/MPQA/train.tsv\")\n",
    "\n",
    "test_path_ls.append(\"./DataSet/MR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/CR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SST/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SUBJ/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/MPQA/test.tsv\")\n",
    "\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/CR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SST_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SUBJ_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MPQA_word_vector.txt\")\n",
    "\n",
    "# 要训练的任务级路径\n",
    "train_path = train_path_ls[DATASET_NUMBER]\n",
    "test_path = test_path_ls[DATASET_NUMBER]\n",
    "\n",
    "# 词向量和W2Idx的路径\n",
    "word_vector_path = word_vector_path_ls[DATASET_NUMBER]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0+cu118\n",
      "DATA_SET: MPQA\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "source": [
    "word_dim = 100 # 词向量维度\n",
    "# 获取单词到索引的映射表以及每个单词的词向量表\n",
    "\n",
    "word_to_index = {'<unknown>': 0, '<padded>': 1}  # 根据筛选出来的词向量文件\"word_vector.txt\" 生成单词和索引的字典\n",
    "zero_ls = [0.0 for i in range(word_dim)]\n",
    "ls = [zero_ls, zero_ls]  # 用一个列表ls来存储词向量 前两个分别是 100 维的0向量，用来表示unknown_token和pad_token\n",
    "index_to_word = {}\n",
    "with open(word_vector_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # 将刑如 \"the -0.038194 -0.24487 0.72812 -0.39961...\"的字符串分成对应的词和词向量，比如\"the\"， \"-0.038194 -0.24487 0.72812 -0.39961...\"\n",
    "        #并且构建单词映射到整数的字典，word_to_index。 eg. 'the' : 2  ，PS：0和1分别是两个特殊字符串 unknown 和 padded的索引，the作为词向量文件中的第一个单词，所以下标为2\n",
    "        word_vector = line.split()\n",
    "        word_to_index[word_vector[0]] = i + 2  # 前两位由unknown和 padded分别占据\n",
    "        tmp = [] # 存储一个单词的词向量，总共是100个数字\n",
    "        for j, word in enumerate(word_vector):\n",
    "            if j == 0: #第一个是单词，所以跳过，只需要每个单词后面的词向量\n",
    "                continue\n",
    "            tmp.append(float(word))\n",
    "        ls.append(tmp) #每个单词的词向量又存到列表ls当中\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "word_vector_weight_matrix = torch.FloatTensor(ls) #将词向量列表转换为Tensor\n",
    "print(word_vector_weight_matrix.size())\n",
    "print(word_vector_weight_matrix.shape[0])\n",
    "print(len(word_to_index))#word_to_index字典的关键字是英语单词，值是单词索引，例如{'<unknown>': 0,'<padded>': 1,'the': 2,',': 3,'.': 4,...}\n",
    "VOCAB_SIZE = len(word_to_index) + 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5568, 100])\n",
      "5568\n",
      "5568\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# 导入AFINN情感词典\n",
    "# from afinn import Afinn\n",
    "\n",
    "# 创建Afinn对象\n",
    "# afinn = Afinn()\n",
    "\n",
    "# 1、【将句子分词，并且求句子长度】\n",
    "def get_sentences(path):\n",
    "    sentences = []\n",
    "    batch_sentences = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # 跳过第一行的test label\n",
    "            if i == 0:\n",
    "                continue\n",
    "            words = line.split()[:-1]  # 需要将末尾的0或者1去掉\n",
    "            label = float(line.split()[-1])\n",
    "            count = 0  # 统计每个句子的长度\n",
    "            for word in words:\n",
    "                count += 1\n",
    "            tmp = (words, count, label)\n",
    "            sentences.append(tmp)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# 根据上面得到的word_to_index,将单词转换为数字,列表形式为[ ([words_list1], len1, label1), ([words_list2], len2, label2),......],\n",
    "# word_list是多个单词组成的序列\n",
    "\n",
    "# 2、【将分词之后的单词序列变成数字序列】\n",
    "def lookup_table(array_ls, word_to_index):\n",
    "    sentences_tensor = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tensor_ls = []\n",
    "        # sentence[0]是一个包含多个单词的列表。sentence[0]是一个列表，里面的元素是单词。\n",
    "        for word in sentence[0]:\n",
    "            # word_to_index 是一个单词到索引的字典\n",
    "            if word in word_to_index.keys():\n",
    "                tensor_ls.append(word_to_index[word])  # 将单词转换为索引，并且索引存入张量当中\n",
    "            else:\n",
    "                tensor_ls.append(1)  # 如果在索引表中没找到单词，则“不认识”单词,用1下标代替, 此时unknown_token和padded_token下标都是1\n",
    "        sentences_tensor.append((tensor_ls, sentence[1], sentence[2]))\n",
    "    return sentences_tensor  # 得到的列表形式为[([3,8,12,123,2,9],6,1),([6,45,652345,634,2342,4],6,0)]\n",
    "\n",
    "\n",
    "# 对句子进行填充，eg:16个句子组成一个batch,【每个batch的句子长度必须相等】，这里的做法是，获取batch中长度最长的句子,然后句子长度不够的append 1\n",
    "# 输入 [([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. [([1, 2, 3],3, 0), ([2, 3], 2, 1), ...]\n",
    "# 输出，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. 假设batch_size = 2\n",
    "# [([1, 2, 3], 3), ([2, 3, 1], 2),...] 第二个句子列表 append 1，但是实际长度为2\n",
    "\n",
    "# 3、【将句子进行分batch，并且将数字列表（句子）进行填充】\n",
    "def pad_sentence_plus(array_ls, array_sentiment, batch_size):  # 这里的array_ls是上面函数的输出，即sentences_tensor。\n",
    "    ans = 0  # j记录每个batch的第一条数据的下标\n",
    "    max = array_ls[0][1]  # max为每个batch的句子最大长度.\n",
    "    # 第一维为列表的第几个元素（即一个句子元组），第二维为一个元组的第一个元素（即一个元组里面的word_list），这里max初始化为第一个句子的长度。\n",
    "    for i in range(len(array_ls)):\n",
    "        # 需要考虑最后一个batch可能长度不够batch_size\n",
    "        if (i + 1) % batch_size == 0 or i == len(array_ls) - 1:\n",
    "            if array_ls[i][1] > max:\n",
    "                max = array_ls[i][1]\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                index = batch_size\n",
    "            else:\n",
    "                index = i - ans + 1\n",
    "            for j in range(index):\n",
    "                while len(array_ls[j + ans][0]) < max:\n",
    "                    array_ls[j + ans][0].append(1)\n",
    "                    array_sentiment[j + ans] += [0]\n",
    "            # 每一次填充完毕后，需要更新标记，并再次初始化最大值\n",
    "            ans = i + 1\n",
    "            if ans != len(array_ls):\n",
    "                max = array_ls[ans][1]\n",
    "        else:\n",
    "            if array_ls[i][1] > max:  # 取句子的最大长度\n",
    "                max = array_ls[i][1]\n",
    "    return array_ls, array_sentiment\n",
    "\n",
    "\n",
    "# 输入，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# [([1, 2, 3],3), ([2, 3, 1], 2),...]\n",
    "# shuffle表示是否将每个batch打乱\n",
    "# batch_first if false 表示返回的张量形状为(sentence_len,batch_size)\n",
    "# if true 表示返回的张量形状为(batch_size,sentence_len)\n",
    "# 这里默认采用batch_first = False,主要是为了适应nn.Embedding层的输入形状\n",
    "\n",
    "# 4、【获得embedding的一个个batch（调整batch的的形状）的数据迭代器】\n",
    "def iterator(array_ls, array_sentiment, batch_size, shuffle=True, batch_first=False):\n",
    "    sentences_index_label_sentiment = []  # 存放填充后的语句列表【句子的indx表示】\n",
    "    tmp_sen = []\n",
    "    tmp_label = []\n",
    "    tmp_sentiment = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tmp_sen.append(sentence[0])  # 存放一个batch的【数据】\n",
    "        tmp_label.append(sentence[2])  # 存放一个batch的【标签】\n",
    "        tmp_sentiment.append(array_sentiment[i])\n",
    "        if (i + 1) % batch_size == 0:  #\n",
    "            sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "            tmp_sen = []  # 清空数据\n",
    "            tmp_label = []  # 清空标签\n",
    "            tmp_sentiment = []\n",
    "\n",
    "    # 最后几个样本可能不够一个batch,需要额外判断\n",
    "    if len(tmp_sen) != 0:\n",
    "        # # 补齐\n",
    "        # for _ in range(batch_size - len(tmp_sen)):\n",
    "        #     tmp_sen.append(sentence[0])\n",
    "        #     tmp_label.append(sentence[2])\n",
    "        #     tmp_sentiment.append(array_sentiment[1])\n",
    "        sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "        # # OR去掉\n",
    "        # break\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(sentences_index_label_sentiment)  # 打乱列表中各个batch的顺序\n",
    "    res = []\n",
    "    # 2D张量转置\n",
    "    # 2D张量转置\n",
    "    if batch_first == False:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]),\n",
    "                        torch.FloatTensor(batch[2]).t()))  # 函数t()求矩阵的转置\n",
    "    else:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]), torch.FloatTensor(batch[2]).t()))\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet, sentiwordnet\n",
    "# 下载 SentiWordNet 数据\n",
    "# nltk.download('sentiwordnet')\n",
    "# 加载 SentiWordNet 数据\n",
    "swn_file_path = nltk.data.find('corpora/sentiwordnet/SentiWordNet_3.0.0.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "source": [
    "# =====================================================================================\n",
    "# ================================= sentiwordnet ======================================\n",
    "# =====================================================================================\n",
    "\n",
    "\n",
    "def get_sentiment_score(word):\n",
    "    \"\"\"获取WordNet情感分数\"\"\"\n",
    "\n",
    "    synsets = list(wordnet.synsets(word))\n",
    "    if synsets:\n",
    "        sentiment_scores = [\n",
    "            sentiwordnet.senti_synset(s.name()).pos_score() - sentiwordnet.senti_synset(s.name()).neg_score()\n",
    "            for s in synsets\n",
    "        ]\n",
    "        avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "        return avg_sentiment_score\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def get_word_sentiment_scores(tokenized_sentence):\n",
    "    \"\"\"计算已分词句子中每个单词的情感极性分数\"\"\"\n",
    "    # 处理否定词\n",
    "    word_scores = [get_sentiment_score(word) for word in tokenized_sentence]\n",
    "    return word_scores\n",
    "\n",
    "\n",
    "def get_batch_word_sentiment_scores(batch_tokenized_sentences):\n",
    "    \"\"\"处理一批已分词的文本并返回其中每个单词的情感极性分数\"\"\"\n",
    "    batch_word_scores = []\n",
    "    for sentence, _, _ in batch_tokenized_sentences:\n",
    "        batch_word_scores.append(get_word_sentiment_scores(sentence))\n",
    "    return batch_word_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED) # 为CPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.cuda.manual_seed(SEED) # 为GPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_NUMBER}' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "source": [
    "def get_iterator(path, batch_size, word_to_index):\n",
    "    # ['another', 'thing', 'i', 'don', \"'t\", 'like', 'is', 'that', 'some', 'of', 'the',\n",
    "    # 'features', 'take', '1-2', 'seconds', 'to', 'load', 'up', '.']\n",
    "    sentences = get_sentences(path)\n",
    "    # ([2, 3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
    "    # 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 19, 0.0)\n",
    "    # 将单词变成indx\n",
    "    sentences_indx = lookup_table(sentences, word_to_index)\n",
    "\n",
    "    # get sentiment polarity by 【AFINN】\n",
    "    # afinn = Afinn()\n",
    "    # sentiment_sentences_AFINN = get_batch_word_sentiment_scores_AFINN(sentences,afinn)\n",
    "    # sentiment_sentences_AFINN = calculate_sentiment_for_phrases(sentences, afinn, n=3)\n",
    "\n",
    "    # get sentiment polarity by 【sentiwordnet】\n",
    "    sentiment_sentences_sentiwordnet = get_batch_word_sentiment_scores(sentences)\n",
    "    sentences_padded, sentiment_sentences_padded = pad_sentence_plus(sentences_indx, sentiment_sentences_sentiwordnet,\n",
    "                                                                     batch_size)\n",
    "\n",
    "    # 把分成一个个batch，然后每个list之内有数据和标签\n",
    "    Iterator_with_sentiments = iterator(sentences_padded, sentiment_sentences_padded, batch_size)\n",
    "\n",
    "    return Iterator_with_sentiments\n",
    "\n",
    "# 2、生成对应任务的句子的词向量\n",
    "train_iterator = get_iterator(train_path, BATCH_SIZE, word_to_index)\n",
    "test_iterator = get_iterator(test_path, BATCH_SIZE, word_to_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "source": [
    "class projection_Euler(nn.Module):\n",
    "    def __init__(self, Embedding, num_layers):\n",
    "        super(projection_Euler, self).__init__()  # projection是子类名\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        amplitude = inputs[0]\n",
    "        phase = inputs[1]\n",
    "        # ================ 变换形状和归一化处理 ================\n",
    "        # -----\n",
    "        amplitude_permute = amplitude.permute(1, 0, 2)\n",
    "        amplitude_norm = F.normalize(amplitude_permute, 2, 2)\n",
    "        # ----\n",
    "        phase_permute = phase.permute(1, 0, 2)\n",
    "\n",
    "        # ================ 计算Am和Ph分别计算实部和虚部 ================\n",
    "        # --- 求欧拉展开式\n",
    "        real_part = amplitude_norm * torch.cos(phase_permute)\n",
    "        imag_part = amplitude_norm * torch.sin(phase_permute)\n",
    "\n",
    "        return [real_part, imag_part]\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixMean(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixMean, self).__init__()\n",
    "        self.mapping_Diagonal_q = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.mapping_Diagonal_k = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.embed_size = Embedding_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        Diagonal1 = inputs[0]\n",
    "        Diagonal2 = inputs[1]\n",
    "        Matrix = inputs[2]\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attention_scores = Diagonal1 * Diagonal2\n",
    "        # 迹求和\n",
    "        attention_weight = torch.sum(attention_scores, dim=-1)\n",
    "\n",
    "        # 归一化\n",
    "        attention = F.softmax(attention_weight, dim=-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # 加权求和\n",
    "        Matrix_output = torch.sum(Matrix * attention, dim=1)\n",
    "\n",
    "        return Matrix_output\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixQ(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixQ, self).__init__()\n",
    "        self.Q_Linear_real1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.Q_Linear_real2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.AttentionReal = projection_CalculateMatrixMean(Embedding_dim)\n",
    "        self.AttentionImag = projection_CalculateMatrixMean(Embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_real = inputs[0]  # 实部\n",
    "        input_imag = inputs[1]  # 虚部\n",
    "        # ==================== 1 计算对角线和密度矩阵 ====================\n",
    "\n",
    "        # ======= 1.1 计算对角线 =======\n",
    "        input_real1 = self.Q_Linear_real1(input_real)\n",
    "        input_imag1 = self.Q_Linear_imag1(input_imag)\n",
    "\n",
    "        # 获得[实部/虚部]对角线向量\n",
    "        diagonal_real1 = input_real1 * input_real1 - input_imag1 * input_imag1\n",
    "        diagonal_imag1 = input_real1 * input_imag1 + input_imag1 * input_real1\n",
    "\n",
    "        # ======= 1.2 计算密度矩阵 =======\n",
    "        input_real2 = self.Q_Linear_real2(input_real)\n",
    "        input_imag2 = self.Q_Linear_imag2(input_imag)\n",
    "\n",
    "        diagonal_real2 = input_real2 * input_real2 - input_imag2 * input_imag2\n",
    "        diagonal_imag2 = input_real2 * input_imag2 + input_imag2 * input_real2\n",
    "\n",
    "        real_part_expand = torch.unsqueeze(input_real2, dim=3)\n",
    "        imag_part_expand = torch.unsqueeze(input_imag2, dim=3)\n",
    "\n",
    "        real_part_expand_transpose = real_part_expand.permute(0, 1, 3, 2)\n",
    "        imag_part_expand_transpose = imag_part_expand.permute(0, 1, 3, 2)\n",
    "\n",
    "        # real 密度矩阵\n",
    "        Matrix_real = torch.matmul(real_part_expand, real_part_expand_transpose) - torch.matmul(imag_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "        # imag 密度矩阵\n",
    "        Matrix_imag = torch.matmul(imag_part_expand, real_part_expand_transpose) + torch.matmul(real_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "\n",
    "        Matrix_Real_output = self.AttentionReal([diagonal_real1, diagonal_real2, Matrix_real])\n",
    "        Matrix_imag_output = self.AttentionImag([diagonal_imag1, diagonal_imag2, Matrix_imag])\n",
    "\n",
    "        return [Matrix_Real_output, Matrix_imag_output]\n",
    "\n",
    "\n",
    "class projection_Measurement(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_Measurement, self).__init__()  # projection是子类名\n",
    "        # 使得tensor是正交的,正交初始化,projector是子类projection的新属性。\n",
    "        self.projector = nn.init.orthogonal_(Parameter(torch.Tensor(2, Embedding_dim, 1)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        v_real_avg = inputs[0]\n",
    "        v_imag_avg = inputs[1]\n",
    "\n",
    "        p_real = self.projector[0]\n",
    "        p_imag = self.projector[1]\n",
    "\n",
    "        p_real_norm = p_real / torch.norm(p_real, dim=0)\n",
    "        p_imag_norm = p_imag / torch.norm(p_imag, dim=0)\n",
    "\n",
    "        p_real_mat = torch.matmul(p_real_norm, p_real_norm.permute(1, 0))\n",
    "        p_imag_mat = torch.matmul(p_imag_norm, p_imag_norm.permute(1, 0))\n",
    "\n",
    "        Pv_real = torch.matmul(v_real_avg, p_real_mat) - torch.matmul(v_imag_avg, p_imag_mat)\n",
    "        Pv_imag = torch.matmul(v_real_avg, p_imag_mat) + torch.matmul(v_imag_avg, p_real_mat)\n",
    "\n",
    "        Pv_real_plus = torch.unsqueeze(Pv_real, dim=1)\n",
    "        Pv_imag_plus = torch.unsqueeze(Pv_imag, dim=1)\n",
    "\n",
    "        return [Pv_real_plus, Pv_imag_plus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "source": [
    "# 映射成三个不同的向量的自注意力机制\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.mapping_query = nn.init.uniform_(\n",
    "            Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))  # 从均匀分布U(a, b)中生成值（随机生成），填充输入的张量或变量\n",
    "        self.mapping_key = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "        self.mapping_value = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = torch.matmul(inputs.permute(1, 0, 2), self.mapping_query)  # permute函数为换维函数，参数为(0，1，2)\n",
    "        key = torch.matmul(inputs.permute(1, 0, 2), self.mapping_key)  # matmul是tensor矩阵乘法\n",
    "        value = torch.matmul(inputs.permute(1, 0, 2), self.mapping_value)\n",
    "\n",
    "        query = query / torch.norm(query, dim=2).reshape(query.shape[0], query.shape[1],\n",
    "                                                         1)  # query.shape[0]=16,query.shape[1]=sentence_length.\n",
    "        key = key / torch.norm(key, dim=2).reshape(key.shape[0], key.shape[1],\n",
    "                                                   1)  # shape of tensor:[16,sentence_length,1]\n",
    "\n",
    "        value = value / torch.norm(value, dim=2).reshape(value.shape[0], value.shape[1], 1)  # query,key,value的形状是一样的\n",
    "\n",
    "        xx = torch.matmul(query, torch.transpose(key, 1, 2)) / np.sqrt(inputs.shape[1])\n",
    "        xx1 = torch.softmax(xx, dim=2)\n",
    "        out = torch.matmul(xx1, value)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "source": [
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, Embedding_dim, num_layers):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "\n",
    "        # GRU layer\n",
    "        self.rnn = nn.GRU(Embedding_dim, Embedding_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "        # self.scale_layer = nn.Linear(2 * Embedding_dim, Embedding_dim)\n",
    "\n",
    "        # Linear layer for skip connection\n",
    "        self.middle_layer = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through GRU layer\n",
    "        output, _ = self.rnn(x)\n",
    "\n",
    "        # Skip connection: Add output of middle layer to GRU layer output\n",
    "        output = output + self.middle_layer(x)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "source": [
    "# -------------------- 01 Model_best_copy8672_cnn_maxpooling ------------------------\n",
    "print(\"Model_best_copy8672_cnn_maxpooling\")\n",
    "class run_complex_network(nn.Module):\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, output_dim, num_layers, pad_idx, VOCAB_SIZE, batch):\n",
    "        super().__init__()\n",
    "\n",
    "        # ================ 1、词嵌入模块 ================\n",
    "        # ------ 实部，预训练模型 -------\n",
    "        self.amplitude_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "\n",
    "        # self.amplitude_embedding2 = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ------ 虚部，传统模型 -------\n",
    "        self.phase_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "        # self.phase_embedding = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "        # self.phase_embedding_sentiment = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ================ 2、GRU模块 信息提取 ================\n",
    "        # 输出张量的维度(L:sequence_length,N:batch_size,hidden_size)\n",
    "        self.gru1 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "        self.gru2 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "\n",
    "        self.self_attention1 = self_attention(Embedding_dim=embedding_dim)\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "\n",
    "        self.projection_Euler = projection_Euler(Embedding=embedding_dim, num_layers=num_layers)\n",
    "        self.projection_CalculateMatrixattension = projection_CalculateMatrixQ(Embedding_dim=embedding_dim)\n",
    "        self.projection_measurement = projection_Measurement(embedding_dim)\n",
    "\n",
    "        # ================ 4、卷积 ================\n",
    "        self.Conv2dOne = nn.Conv2d(1, 1, 3)  # 实部\n",
    "        self.Conv2dTwo = nn.Conv2d(1, 1, 3)  # 虚部\n",
    "\n",
    "        # ================ 5、池化 ================\n",
    "\n",
    "        self.MaxPool1 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 实部\n",
    "        self.MaxPool2 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 虚部\n",
    "\n",
    "        # ================ 6、全连接层 ================\n",
    "        self.fc1 = nn.Linear(2 * (embedding_dim - 2), 10)\n",
    "        self.fc2 = nn.Linear(10, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # ================ 1、文本嵌入到向量 ================\n",
    "        amplitude_is_WordEmbedding = self.amplitude_embedding(text)\n",
    "\n",
    "        phase_is_sentiment = self.phase_embedding(text)\n",
    "\n",
    "        # ================ 2、实部进行GRU处理 ================\n",
    "        amplitude_plus = self.gru1(amplitude_is_WordEmbedding)\n",
    "        amplitude_plus2 = self.self_attention1(amplitude_plus)\n",
    "        amplitude_plus2 = amplitude_plus2.permute(1, 0, 2)\n",
    "\n",
    "        phase_plus = self.gru2(phase_is_sentiment)\n",
    "\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "        embedded = [amplitude_plus2, phase_plus]\n",
    "\n",
    "        Euler_realAimag = self.projection_Euler(embedded)\n",
    "        Matrix_realAimag = self.projection_CalculateMatrixattension(Euler_realAimag)\n",
    "        Project_realAimag = self.projection_measurement(Matrix_realAimag)\n",
    "\n",
    "        MatrixReal = Project_realAimag[0]\n",
    "        MatrixImag = Project_realAimag[1]\n",
    "\n",
    "        # ================ 4、卷积操作 ================\n",
    "        # 5.1 --对于【实部】进行卷积\n",
    "        Conv_real = self.Conv2dOne(MatrixReal)\n",
    "        # 5.2 --对于【虚部】进行卷积\n",
    "        Conv_imag = self.Conv2dTwo(MatrixImag)\n",
    "\n",
    "        # ================ 5、池化操作 ================\n",
    "        # 6.1 --对于【实部】进行池化\n",
    "        Max_real = self.MaxPool1(torch.sigmoid(Conv_real))\n",
    "        # 6.2 --对于【虚部】进行池化\n",
    "        Max_imag = self.MaxPool2(torch.sigmoid(Conv_imag))\n",
    "\n",
    "        # ================ 6、全连接 ================\n",
    "        fc1 = self.fc1(torch.cat((Max_real, Max_imag), dim=3))\n",
    "        fc2 = torch.sigmoid(self.fc2(torch.sigmoid(fc1)))\n",
    "\n",
    "        return fc2  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_best_copy8672_cnn_maxpooling\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def multiply(input1, input2):\n",
    "    return torch.sum(torch.mul(input1, input2)).item()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    # print('batch: 模型预测值', rounded_preds,'真实标签', y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def calculation_recall(preds, y):\n",
    "    pre = preds.detach()\n",
    "    pre_y = y.detach()\n",
    "    rounded_preds = torch.round(pre)\n",
    "    \n",
    "    TP = (torch.from_numpy(rounded_preds.cpu().numpy().astype(int)) & torch.from_numpy(pre_y.cpu().numpy().astype(int))).sum()\n",
    "    recall = TP / (y.cpu().sum())\n",
    "    return recall\n",
    "\n",
    "def f1_score_avg(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    f1_score(y, rounded_preds, 'macro')\n",
    "\n",
    "# 定义训练流程\n",
    "# 把训练数据投入模型进行训练\n",
    "def train(model, iterator, optimizer, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        text = batch[0].cuda(CUDA_NUMBER)\n",
    "        label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text).squeeze(1)\n",
    "        # print(predictions)\n",
    "        loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "        acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "        recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "        label_score = label.tolist()\n",
    "\n",
    "        y_true = label_score.copy()\n",
    "\n",
    "        predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "\n",
    "        y_pred = predictions_score.copy()\n",
    "\n",
    "        F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        epoch_recall += recall.item()\n",
    "\n",
    "        epoch_f1_score += F1Score.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "# 评价\n",
    "def evaluate(model, iterator, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch[0].cuda(CUDA_NUMBER)\n",
    "            label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "            predictions = model(text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "            acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "            recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "            label_score = label.tolist()\n",
    "            y_true = label_score.copy()\n",
    "            predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "            y_pred = predictions_score.copy()\n",
    "            F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1_score += F1Score.item()\n",
    "            epoch_recall += recall.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "num_layers = 2\n",
    "print(\"num_layers\",num_layers)\n",
    "# vocab_size = 19192, embedding_dim = 100, hidden_dim = 3, output_dim = 1, dropout = 0.5, pad_idx = 1\n",
    "PAD_IDX = 1\n",
    "print('pad_idx', PAD_IDX)\n",
    "\n",
    "model = run_complex_network(word_vector_weight_matrix,\n",
    "                            EMBEDDING_DIM,\n",
    "                            HIDDEN_DIM,\n",
    "                            OUTPUT_DIM,\n",
    "                            num_layers,\n",
    "                            PAD_IDX,\n",
    "                            VOCAB_SIZE,\n",
    "                            BATCH_SIZE)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())#优化器使用的是Adam\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "best_test_acc = float('-inf')\n",
    "print(0.5 > best_test_acc)\n",
    "\n",
    "best_acc = -1\n",
    "best_f1_score = -1\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "f1_score_test = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc, train_f1_score, train_recall \\\n",
    "        = train(model, train_iterator, optimizer, criterion, CUDA_NUMBER)\n",
    "\n",
    "    test_loss, test_acc, test_f1_score, test_recall \\\n",
    "        = evaluate(model, test_iterator, criterion, CUDA_NUMBER)\n",
    "\n",
    "    acc_train.append(train_acc)\n",
    "    acc_test.append(test_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    loss_test.append(test_loss)\n",
    "    f1_score_test.append(test_f1_score)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'SST_GRU_Conv_model.pt')\n",
    "\n",
    "    # print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    # print(\n",
    "    #     f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}% | Train f1_score: {train_f1_score * 100:.2f}% | Train recall: {train_recall * 100:.2f}%')\n",
    "    # print(\n",
    "    #     f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc * 100:.2f}% | Test f1_score: {test_f1_score * 100:.2f}%| best_acc: {best_acc * 100:.2f}% | Test recall: {test_recall * 100:.2f}%')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t test  Loss: {test_loss:.3f} | test  Acc: {test_acc * 100:.2f}%')\n",
    "    print(f'\\t best  test acc: {best_test_acc * 100:.2f}%')\n",
    "\n",
    "with open('./train/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_train:\n",
    "        f.write(str(i))\n",
    "    f.close()\n",
    "\n",
    "with open('./test/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/F1_Score/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in f1_score_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/acc/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in acc_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/recall/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in test_path_ls:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "#\n",
    "colors1 = '#00CED1' \n",
    "colors2 = '#DC143C'\n",
    "colors3 = '#FFA500'\n",
    "colors4 = '#0000aFF'\n",
    "colors5 = '#FF0000'\n",
    "\n",
    "# 画初始点\n",
    "plt.figure(1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(DATA_SET + ' Loss')\n",
    "plt.plot(range(N_EPOCHS), acc_train[0:N_EPOCHS], 'r-o', label='train_loss')\n",
    "plt.plot(range(N_EPOCHS), acc_test[0:N_EPOCHS], 'c-*', label='test_loss')\n",
    "plt.xlim(-0.01, N_EPOCHS)\n",
    "plt.ylim(-0.01, 1)\n",
    "# plt.plot(range(2), sp, c=colors3)\n",
    "plt.legend()\n",
    "plt.savefig('pic' + DATA_SET + '.pdf', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num_layers 2\n",
      "pad_idx 1\n",
      "The model has 1,489,201 trainable parameters\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.510 | Train Acc: 77.45%\n",
      "\t test  Loss: 0.378 | test  Acc: 86.38%\n",
      "\t best  test acc: 86.38%\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.315 | Train Acc: 89.08%\n",
      "\t test  Loss: 0.372 | test  Acc: 85.45%\n",
      "\t best  test acc: 86.38%\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.241 | Train Acc: 92.35%\n",
      "\t test  Loss: 0.342 | test  Acc: 86.29%\n",
      "\t best  test acc: 86.38%\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.200 | Train Acc: 93.99%\n",
      "\t test  Loss: 0.361 | test  Acc: 86.94%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.197 | Train Acc: 94.10%\n",
      "\t test  Loss: 0.379 | test  Acc: 83.21%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.169 | Train Acc: 94.92%\n",
      "\t test  Loss: 0.402 | test  Acc: 83.30%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.150 | Train Acc: 95.47%\n",
      "\t test  Loss: 0.386 | test  Acc: 86.19%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.138 | Train Acc: 95.82%\n",
      "\t test  Loss: 0.424 | test  Acc: 82.37%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.123 | Train Acc: 96.03%\n",
      "\t test  Loss: 0.424 | test  Acc: 84.24%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.113 | Train Acc: 96.37%\n",
      "\t test  Loss: 0.460 | test  Acc: 82.65%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.110 | Train Acc: 96.25%\n",
      "\t test  Loss: 0.475 | test  Acc: 80.60%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.099 | Train Acc: 96.80%\n",
      "\t test  Loss: 0.535 | test  Acc: 82.09%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.086 | Train Acc: 97.32%\n",
      "\t test  Loss: 0.490 | test  Acc: 85.07%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.082 | Train Acc: 97.29%\n",
      "\t test  Loss: 0.542 | test  Acc: 81.90%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.081 | Train Acc: 97.29%\n",
      "\t test  Loss: 0.517 | test  Acc: 82.74%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.070 | Train Acc: 97.45%\n",
      "\t test  Loss: 0.572 | test  Acc: 81.72%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.071 | Train Acc: 97.30%\n",
      "\t test  Loss: 0.528 | test  Acc: 85.07%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.059 | Train Acc: 97.87%\n",
      "\t test  Loss: 0.619 | test  Acc: 80.88%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 19 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.057 | Train Acc: 97.79%\n",
      "\t test  Loss: 0.602 | test  Acc: 81.72%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 20 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.050 | Train Acc: 97.98%\n",
      "\t test  Loss: 0.640 | test  Acc: 80.13%\n",
      "\t best  test acc: 86.94%\n",
      "Epoch: 21 | Epoch Time: 0m 8s\n",
      "\t Train Loss: 0.045 | Train Acc: 98.18%\n",
      "\t test  Loss: 0.613 | test  Acc: 82.46%\n",
      "\t best  test acc: 86.94%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "print(torch.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DATA_SET = 'SUBJ'\n",
    "N_EPOCHS = 26\n",
    "\n",
    "print(\"DATA_SET:\", DATA_SET)\n",
    "CUDA_NUMBER = 0\n",
    "\n",
    "# 0-->MR ||  1-->CR ||  2-->SST  ||  3-->SUBJ ||  4-->MPQA\n",
    "dataset_dict = {}\n",
    "dataset_dict['MR'] = 0\n",
    "dataset_dict['CR'] = 1\n",
    "dataset_dict['SST'] = 2\n",
    "dataset_dict['SUBJ'] = 3\n",
    "dataset_dict['MPQA'] = 4\n",
    "\n",
    "# 设置要训练的数据集\n",
    "DATASET_NUMBER = dataset_dict[DATA_SET]\n",
    "\n",
    "train_path_ls = []\n",
    "test_path_ls = []\n",
    "word_vector_path_ls = []\n",
    "train_path_ls.append(\"./DataSet/MR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/CR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SST/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SUBJ/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/MPQA/train.tsv\")\n",
    "\n",
    "test_path_ls.append(\"./DataSet/MR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/CR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SST/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SUBJ/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/MPQA/test.tsv\")\n",
    "\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/CR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SST_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SUBJ_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MPQA_word_vector.txt\")\n",
    "\n",
    "# 要训练的任务级路径\n",
    "train_path = train_path_ls[DATASET_NUMBER]\n",
    "test_path = test_path_ls[DATASET_NUMBER]\n",
    "\n",
    "# 词向量和W2Idx的路径\n",
    "word_vector_path = word_vector_path_ls[DATASET_NUMBER]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0+cu118\n",
      "DATA_SET: SUBJ\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "word_dim = 100 # 词向量维度\n",
    "# 获取单词到索引的映射表以及每个单词的词向量表\n",
    "\n",
    "word_to_index = {'<unknown>': 0, '<padded>': 1}  # 根据筛选出来的词向量文件\"word_vector.txt\" 生成单词和索引的字典\n",
    "zero_ls = [0.0 for i in range(word_dim)]\n",
    "ls = [zero_ls, zero_ls]  # 用一个列表ls来存储词向量 前两个分别是 100 维的0向量，用来表示unknown_token和pad_token\n",
    "index_to_word = {}\n",
    "with open(word_vector_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # 将刑如 \"the -0.038194 -0.24487 0.72812 -0.39961...\"的字符串分成对应的词和词向量，比如\"the\"， \"-0.038194 -0.24487 0.72812 -0.39961...\"\n",
    "        #并且构建单词映射到整数的字典，word_to_index。 eg. 'the' : 2  ，PS：0和1分别是两个特殊字符串 unknown 和 padded的索引，the作为词向量文件中的第一个单词，所以下标为2\n",
    "        word_vector = line.split()\n",
    "        word_to_index[word_vector[0]] = i + 2  # 前两位由unknown和 padded分别占据\n",
    "        tmp = [] # 存储一个单词的词向量，总共是100个数字\n",
    "        for j, word in enumerate(word_vector):\n",
    "            if j == 0: #第一个是单词，所以跳过，只需要每个单词后面的词向量\n",
    "                continue\n",
    "            tmp.append(float(word))\n",
    "        ls.append(tmp) #每个单词的词向量又存到列表ls当中\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "word_vector_weight_matrix = torch.FloatTensor(ls) #将词向量列表转换为Tensor\n",
    "print(word_vector_weight_matrix.size())\n",
    "print(word_vector_weight_matrix.shape[0])\n",
    "print(len(word_to_index))#word_to_index字典的关键字是英语单词，值是单词索引，例如{'<unknown>': 0,'<padded>': 1,'the': 2,',': 3,'.': 4,...}\n",
    "VOCAB_SIZE = len(word_to_index) + 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([18512, 100])\n",
      "18512\n",
      "18512\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# 导入AFINN情感词典\n",
    "# from afinn import Afinn\n",
    "\n",
    "# 创建Afinn对象\n",
    "# afinn = Afinn()\n",
    "\n",
    "# 1、【将句子分词，并且求句子长度】\n",
    "def get_sentences(path):\n",
    "    sentences = []\n",
    "    batch_sentences = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # 跳过第一行的test label\n",
    "            if i == 0:\n",
    "                continue\n",
    "            words = line.split()[:-1]  # 需要将末尾的0或者1去掉\n",
    "            label = float(line.split()[-1])\n",
    "            count = 0  # 统计每个句子的长度\n",
    "            for word in words:\n",
    "                count += 1\n",
    "            tmp = (words, count, label)\n",
    "            sentences.append(tmp)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# 根据上面得到的word_to_index,将单词转换为数字,列表形式为[ ([words_list1], len1, label1), ([words_list2], len2, label2),......],\n",
    "# word_list是多个单词组成的序列\n",
    "\n",
    "# 2、【将分词之后的单词序列变成数字序列】\n",
    "def lookup_table(array_ls, word_to_index):\n",
    "    sentences_tensor = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tensor_ls = []\n",
    "        # sentence[0]是一个包含多个单词的列表。sentence[0]是一个列表，里面的元素是单词。\n",
    "        for word in sentence[0]:\n",
    "            # word_to_index 是一个单词到索引的字典\n",
    "            if word in word_to_index.keys():\n",
    "                tensor_ls.append(word_to_index[word])  # 将单词转换为索引，并且索引存入张量当中\n",
    "            else:\n",
    "                tensor_ls.append(1)  # 如果在索引表中没找到单词，则“不认识”单词,用1下标代替, 此时unknown_token和padded_token下标都是1\n",
    "        sentences_tensor.append((tensor_ls, sentence[1], sentence[2]))\n",
    "    return sentences_tensor  # 得到的列表形式为[([3,8,12,123,2,9],6,1),([6,45,652345,634,2342,4],6,0)]\n",
    "\n",
    "\n",
    "# 对句子进行填充，eg:16个句子组成一个batch,【每个batch的句子长度必须相等】，这里的做法是，获取batch中长度最长的句子,然后句子长度不够的append 1\n",
    "# 输入 [([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. [([1, 2, 3],3, 0), ([2, 3], 2, 1), ...]\n",
    "# 输出，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. 假设batch_size = 2\n",
    "# [([1, 2, 3], 3), ([2, 3, 1], 2),...] 第二个句子列表 append 1，但是实际长度为2\n",
    "\n",
    "# 3、【将句子进行分batch，并且将数字列表（句子）进行填充】\n",
    "def pad_sentence_plus(array_ls, array_sentiment, batch_size):  # 这里的array_ls是上面函数的输出，即sentences_tensor。\n",
    "    ans = 0  # j记录每个batch的第一条数据的下标\n",
    "    max = array_ls[0][1]  # max为每个batch的句子最大长度.\n",
    "    # 第一维为列表的第几个元素（即一个句子元组），第二维为一个元组的第一个元素（即一个元组里面的word_list），这里max初始化为第一个句子的长度。\n",
    "    for i in range(len(array_ls)):\n",
    "        # 需要考虑最后一个batch可能长度不够batch_size\n",
    "        if (i + 1) % batch_size == 0 or i == len(array_ls) - 1:\n",
    "            if array_ls[i][1] > max:\n",
    "                max = array_ls[i][1]\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                index = batch_size\n",
    "            else:\n",
    "                index = i - ans + 1\n",
    "            for j in range(index):\n",
    "                while len(array_ls[j + ans][0]) < max:\n",
    "                    array_ls[j + ans][0].append(1)\n",
    "                    array_sentiment[j + ans] += [0]\n",
    "            # 每一次填充完毕后，需要更新标记，并再次初始化最大值\n",
    "            ans = i + 1\n",
    "            if ans != len(array_ls):\n",
    "                max = array_ls[ans][1]\n",
    "        else:\n",
    "            if array_ls[i][1] > max:  # 取句子的最大长度\n",
    "                max = array_ls[i][1]\n",
    "    return array_ls, array_sentiment\n",
    "\n",
    "\n",
    "# 输入，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# [([1, 2, 3],3), ([2, 3, 1], 2),...]\n",
    "# shuffle表示是否将每个batch打乱\n",
    "# batch_first if false 表示返回的张量形状为(sentence_len,batch_size)\n",
    "# if true 表示返回的张量形状为(batch_size,sentence_len)\n",
    "# 这里默认采用batch_first = False,主要是为了适应nn.Embedding层的输入形状\n",
    "\n",
    "# 4、【获得embedding的一个个batch（调整batch的的形状）的数据迭代器】\n",
    "def iterator(array_ls, array_sentiment, batch_size, shuffle=True, batch_first=False):\n",
    "    sentences_index_label_sentiment = []  # 存放填充后的语句列表【句子的indx表示】\n",
    "    tmp_sen = []\n",
    "    tmp_label = []\n",
    "    tmp_sentiment = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tmp_sen.append(sentence[0])  # 存放一个batch的【数据】\n",
    "        tmp_label.append(sentence[2])  # 存放一个batch的【标签】\n",
    "        tmp_sentiment.append(array_sentiment[i])\n",
    "        if (i + 1) % batch_size == 0:  #\n",
    "            sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "            tmp_sen = []  # 清空数据\n",
    "            tmp_label = []  # 清空标签\n",
    "            tmp_sentiment = []\n",
    "\n",
    "    # 最后几个样本可能不够一个batch,需要额外判断\n",
    "    if len(tmp_sen) != 0:\n",
    "        # # 补齐\n",
    "        # for _ in range(batch_size - len(tmp_sen)):\n",
    "        #     tmp_sen.append(sentence[0])\n",
    "        #     tmp_label.append(sentence[2])\n",
    "        #     tmp_sentiment.append(array_sentiment[1])\n",
    "        sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "        # # OR去掉\n",
    "        # break\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(sentences_index_label_sentiment)  # 打乱列表中各个batch的顺序\n",
    "    res = []\n",
    "    # 2D张量转置\n",
    "    # 2D张量转置\n",
    "    if batch_first == False:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]),\n",
    "                        torch.FloatTensor(batch[2]).t()))  # 函数t()求矩阵的转置\n",
    "    else:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]), torch.FloatTensor(batch[2]).t()))\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet, sentiwordnet\n",
    "# 下载 SentiWordNet 数据\n",
    "# nltk.download('sentiwordnet')\n",
    "# 加载 SentiWordNet 数据\n",
    "swn_file_path = nltk.data.find('corpora/sentiwordnet/SentiWordNet_3.0.0.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "source": [
    "# =====================================================================================\n",
    "# ================================= sentiwordnet ======================================\n",
    "# =====================================================================================\n",
    "\n",
    "\n",
    "def get_sentiment_score(word):\n",
    "    \"\"\"获取WordNet情感分数\"\"\"\n",
    "\n",
    "    synsets = list(wordnet.synsets(word))\n",
    "    if synsets:\n",
    "        sentiment_scores = [\n",
    "            sentiwordnet.senti_synset(s.name()).pos_score() - sentiwordnet.senti_synset(s.name()).neg_score()\n",
    "            for s in synsets\n",
    "        ]\n",
    "        avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "        return avg_sentiment_score\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def get_word_sentiment_scores(tokenized_sentence):\n",
    "    \"\"\"计算已分词句子中每个单词的情感极性分数\"\"\"\n",
    "    # 处理否定词\n",
    "    word_scores = [get_sentiment_score(word) for word in tokenized_sentence]\n",
    "    return word_scores\n",
    "\n",
    "\n",
    "def get_batch_word_sentiment_scores(batch_tokenized_sentences):\n",
    "    \"\"\"处理一批已分词的文本并返回其中每个单词的情感极性分数\"\"\"\n",
    "    batch_word_scores = []\n",
    "    for sentence, _, _ in batch_tokenized_sentences:\n",
    "        batch_word_scores.append(get_word_sentiment_scores(sentence))\n",
    "    return batch_word_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED) # 为CPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.cuda.manual_seed(SEED) # 为GPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_NUMBER}' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "source": [
    "def get_iterator(path, batch_size, word_to_index):\n",
    "    # ['another', 'thing', 'i', 'don', \"'t\", 'like', 'is', 'that', 'some', 'of', 'the',\n",
    "    # 'features', 'take', '1-2', 'seconds', 'to', 'load', 'up', '.']\n",
    "    sentences = get_sentences(path)\n",
    "    # ([2, 3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
    "    # 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 19, 0.0)\n",
    "    # 将单词变成indx\n",
    "    sentences_indx = lookup_table(sentences, word_to_index)\n",
    "\n",
    "    # get sentiment polarity by 【AFINN】\n",
    "    # afinn = Afinn()\n",
    "    # sentiment_sentences_AFINN = get_batch_word_sentiment_scores_AFINN(sentences,afinn)\n",
    "    # sentiment_sentences_AFINN = calculate_sentiment_for_phrases(sentences, afinn, n=3)\n",
    "\n",
    "    # get sentiment polarity by 【sentiwordnet】\n",
    "    sentiment_sentences_sentiwordnet = get_batch_word_sentiment_scores(sentences)\n",
    "    sentences_padded, sentiment_sentences_padded = pad_sentence_plus(sentences_indx, sentiment_sentences_sentiwordnet,\n",
    "                                                                     batch_size)\n",
    "\n",
    "    # 把分成一个个batch，然后每个list之内有数据和标签\n",
    "    Iterator_with_sentiments = iterator(sentences_padded, sentiment_sentences_padded, batch_size)\n",
    "\n",
    "    return Iterator_with_sentiments\n",
    "\n",
    "# 2、生成对应任务的句子的词向量\n",
    "train_iterator = get_iterator(train_path, BATCH_SIZE, word_to_index)\n",
    "test_iterator = get_iterator(test_path, BATCH_SIZE, word_to_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "source": [
    "class projection_Euler(nn.Module):\n",
    "    def __init__(self, Embedding, num_layers):\n",
    "        super(projection_Euler, self).__init__()  # projection是子类名\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        amplitude = inputs[0]\n",
    "        phase = inputs[1]\n",
    "        # ================ 变换形状和归一化处理 ================\n",
    "        # -----\n",
    "        amplitude_permute = amplitude.permute(1, 0, 2)\n",
    "        amplitude_norm = F.normalize(amplitude_permute, 2, 2)\n",
    "        # ----\n",
    "        phase_permute = phase.permute(1, 0, 2)\n",
    "\n",
    "        # ================ 计算Am和Ph分别计算实部和虚部 ================\n",
    "        # --- 求欧拉展开式\n",
    "        real_part = amplitude_norm * torch.cos(phase_permute)\n",
    "        imag_part = amplitude_norm * torch.sin(phase_permute)\n",
    "\n",
    "        return [real_part, imag_part]\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixMean(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixMean, self).__init__()\n",
    "        self.mapping_Diagonal_q = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.mapping_Diagonal_k = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.embed_size = Embedding_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        Diagonal1 = inputs[0]\n",
    "        Diagonal2 = inputs[1]\n",
    "        Matrix = inputs[2]\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attention_scores = Diagonal1 * Diagonal2\n",
    "        # 迹求和\n",
    "        attention_weight = torch.sum(attention_scores, dim=-1)\n",
    "\n",
    "        # 归一化\n",
    "        attention = F.softmax(attention_weight, dim=-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # 加权求和\n",
    "        Matrix_output = torch.sum(Matrix * attention, dim=1)\n",
    "\n",
    "        return Matrix_output\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixQ(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixQ, self).__init__()\n",
    "        self.Q_Linear_real1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.Q_Linear_real2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.AttentionReal = projection_CalculateMatrixMean(Embedding_dim)\n",
    "        self.AttentionImag = projection_CalculateMatrixMean(Embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_real = inputs[0]  # 实部\n",
    "        input_imag = inputs[1]  # 虚部\n",
    "        # ==================== 1 计算对角线和密度矩阵 ====================\n",
    "\n",
    "        # ======= 1.1 计算对角线 =======\n",
    "        input_real1 = self.Q_Linear_real1(input_real)\n",
    "        input_imag1 = self.Q_Linear_imag1(input_imag)\n",
    "\n",
    "        # 获得[实部/虚部]对角线向量\n",
    "        diagonal_real1 = input_real1 * input_real1 - input_imag1 * input_imag1\n",
    "        diagonal_imag1 = input_real1 * input_imag1 + input_imag1 * input_real1\n",
    "\n",
    "        # ======= 1.2 计算密度矩阵 =======\n",
    "        input_real2 = self.Q_Linear_real2(input_real)\n",
    "        input_imag2 = self.Q_Linear_imag2(input_imag)\n",
    "\n",
    "        diagonal_real2 = input_real2 * input_real2 - input_imag2 * input_imag2\n",
    "        diagonal_imag2 = input_real2 * input_imag2 + input_imag2 * input_real2\n",
    "\n",
    "        real_part_expand = torch.unsqueeze(input_real2, dim=3)\n",
    "        imag_part_expand = torch.unsqueeze(input_imag2, dim=3)\n",
    "\n",
    "        real_part_expand_transpose = real_part_expand.permute(0, 1, 3, 2)\n",
    "        imag_part_expand_transpose = imag_part_expand.permute(0, 1, 3, 2)\n",
    "\n",
    "        # real 密度矩阵\n",
    "        Matrix_real = torch.matmul(real_part_expand, real_part_expand_transpose) - torch.matmul(imag_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "        # imag 密度矩阵\n",
    "        Matrix_imag = torch.matmul(imag_part_expand, real_part_expand_transpose) + torch.matmul(real_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "\n",
    "        Matrix_Real_output = self.AttentionReal([diagonal_real1, diagonal_real2, Matrix_real])\n",
    "        Matrix_imag_output = self.AttentionImag([diagonal_imag1, diagonal_imag2, Matrix_imag])\n",
    "\n",
    "        return [Matrix_Real_output, Matrix_imag_output]\n",
    "\n",
    "\n",
    "class projection_Measurement(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_Measurement, self).__init__()  # projection是子类名\n",
    "        # 使得tensor是正交的,正交初始化,projector是子类projection的新属性。\n",
    "        self.projector = nn.init.orthogonal_(Parameter(torch.Tensor(2, Embedding_dim, 1)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        v_real_avg = inputs[0]\n",
    "        v_imag_avg = inputs[1]\n",
    "\n",
    "        p_real = self.projector[0]\n",
    "        p_imag = self.projector[1]\n",
    "\n",
    "        p_real_norm = p_real / torch.norm(p_real, dim=0)\n",
    "        p_imag_norm = p_imag / torch.norm(p_imag, dim=0)\n",
    "\n",
    "        p_real_mat = torch.matmul(p_real_norm, p_real_norm.permute(1, 0))\n",
    "        p_imag_mat = torch.matmul(p_imag_norm, p_imag_norm.permute(1, 0))\n",
    "\n",
    "        Pv_real = torch.matmul(v_real_avg, p_real_mat) - torch.matmul(v_imag_avg, p_imag_mat)\n",
    "        Pv_imag = torch.matmul(v_real_avg, p_imag_mat) + torch.matmul(v_imag_avg, p_real_mat)\n",
    "\n",
    "        Pv_real_plus = torch.unsqueeze(Pv_real, dim=1)\n",
    "        Pv_imag_plus = torch.unsqueeze(Pv_imag, dim=1)\n",
    "\n",
    "        return [Pv_real_plus, Pv_imag_plus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "source": [
    "# 映射成三个不同的向量的自注意力机制\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.mapping_query = nn.init.uniform_(\n",
    "            Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))  # 从均匀分布U(a, b)中生成值（随机生成），填充输入的张量或变量\n",
    "        self.mapping_key = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "        self.mapping_value = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = torch.matmul(inputs.permute(1, 0, 2), self.mapping_query)  # permute函数为换维函数，参数为(0，1，2)\n",
    "        key = torch.matmul(inputs.permute(1, 0, 2), self.mapping_key)  # matmul是tensor矩阵乘法\n",
    "        value = torch.matmul(inputs.permute(1, 0, 2), self.mapping_value)\n",
    "\n",
    "        query = query / torch.norm(query, dim=2).reshape(query.shape[0], query.shape[1],\n",
    "                                                         1)  # query.shape[0]=16,query.shape[1]=sentence_length.\n",
    "        key = key / torch.norm(key, dim=2).reshape(key.shape[0], key.shape[1],\n",
    "                                                   1)  # shape of tensor:[16,sentence_length,1]\n",
    "\n",
    "        value = value / torch.norm(value, dim=2).reshape(value.shape[0], value.shape[1], 1)  # query,key,value的形状是一样的\n",
    "\n",
    "        xx = torch.matmul(query, torch.transpose(key, 1, 2)) / np.sqrt(inputs.shape[1])\n",
    "        xx1 = torch.softmax(xx, dim=2)\n",
    "        out = torch.matmul(xx1, value)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "source": [
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, Embedding_dim, num_layers):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "\n",
    "        # GRU layer\n",
    "        self.rnn = nn.GRU(Embedding_dim, Embedding_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "        # self.scale_layer = nn.Linear(2 * Embedding_dim, Embedding_dim)\n",
    "\n",
    "        # Linear layer for skip connection\n",
    "        self.middle_layer = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through GRU layer\n",
    "        output, _ = self.rnn(x)\n",
    "\n",
    "        # Skip connection: Add output of middle layer to GRU layer output\n",
    "        output = output + self.middle_layer(x)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "source": [
    "# -------------------- 01 Model_best_copy8672_cnn_maxpooling ------------------------\n",
    "print(\"Model_best_copy8672_cnn_maxpooling\")\n",
    "class run_complex_network(nn.Module):\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, output_dim, num_layers, pad_idx, VOCAB_SIZE, batch):\n",
    "        super().__init__()\n",
    "\n",
    "        # ================ 1、词嵌入模块 ================\n",
    "        # ------ 实部，预训练模型 -------\n",
    "        self.amplitude_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "\n",
    "        # self.amplitude_embedding2 = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ------ 虚部，传统模型 -------\n",
    "        self.phase_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "        # self.phase_embedding = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "        # self.phase_embedding_sentiment = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ================ 2、GRU模块 信息提取 ================\n",
    "        # 输出张量的维度(L:sequence_length,N:batch_size,hidden_size)\n",
    "        self.gru1 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "        self.gru2 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "\n",
    "        self.self_attention1 = self_attention(Embedding_dim=embedding_dim)\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "\n",
    "        self.projection_Euler = projection_Euler(Embedding=embedding_dim, num_layers=num_layers)\n",
    "        self.projection_CalculateMatrixattension = projection_CalculateMatrixQ(Embedding_dim=embedding_dim)\n",
    "        self.projection_measurement = projection_Measurement(embedding_dim)\n",
    "\n",
    "        # ================ 4、卷积 ================\n",
    "        self.Conv2dOne = nn.Conv2d(1, 1, 3)  # 实部\n",
    "        self.Conv2dTwo = nn.Conv2d(1, 1, 3)  # 虚部\n",
    "\n",
    "        # ================ 5、池化 ================\n",
    "\n",
    "        self.MaxPool1 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 实部\n",
    "        self.MaxPool2 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 虚部\n",
    "\n",
    "        # ================ 6、全连接层 ================\n",
    "        self.fc1 = nn.Linear(2 * (embedding_dim - 2), 10)\n",
    "        self.fc2 = nn.Linear(10, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # ================ 1、文本嵌入到向量 ================\n",
    "        amplitude_is_WordEmbedding = self.amplitude_embedding(text)\n",
    "\n",
    "        phase_is_sentiment = self.phase_embedding(text)\n",
    "\n",
    "        # ================ 2、实部进行GRU处理 ================\n",
    "        amplitude_plus = self.gru1(amplitude_is_WordEmbedding)\n",
    "        amplitude_plus2 = self.self_attention1(amplitude_plus)\n",
    "        amplitude_plus2 = amplitude_plus2.permute(1, 0, 2)\n",
    "\n",
    "        phase_plus = self.gru2(phase_is_sentiment)\n",
    "\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "        embedded = [amplitude_plus2, phase_plus]\n",
    "\n",
    "        Euler_realAimag = self.projection_Euler(embedded)\n",
    "        Matrix_realAimag = self.projection_CalculateMatrixattension(Euler_realAimag)\n",
    "        Project_realAimag = self.projection_measurement(Matrix_realAimag)\n",
    "\n",
    "        MatrixReal = Project_realAimag[0]\n",
    "        MatrixImag = Project_realAimag[1]\n",
    "\n",
    "        # ================ 4、卷积操作 ================\n",
    "        # 5.1 --对于【实部】进行卷积\n",
    "        Conv_real = self.Conv2dOne(MatrixReal)\n",
    "        # 5.2 --对于【虚部】进行卷积\n",
    "        Conv_imag = self.Conv2dTwo(MatrixImag)\n",
    "\n",
    "        # ================ 5、池化操作 ================\n",
    "        # 6.1 --对于【实部】进行池化\n",
    "        Max_real = self.MaxPool1(torch.sigmoid(Conv_real))\n",
    "        # 6.2 --对于【虚部】进行池化\n",
    "        Max_imag = self.MaxPool2(torch.sigmoid(Conv_imag))\n",
    "\n",
    "        # ================ 6、全连接 ================\n",
    "        fc1 = self.fc1(torch.cat((Max_real, Max_imag), dim=3))\n",
    "        fc2 = torch.sigmoid(self.fc2(torch.sigmoid(fc1)))\n",
    "\n",
    "        return fc2  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_best_copy8672_cnn_maxpooling\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def multiply(input1, input2):\n",
    "    return torch.sum(torch.mul(input1, input2)).item()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    # print('batch: 模型预测值', rounded_preds,'真实标签', y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def calculation_recall(preds, y):\n",
    "    pre = preds.detach()\n",
    "    pre_y = y.detach()\n",
    "    rounded_preds = torch.round(pre)\n",
    "    \n",
    "    TP = (torch.from_numpy(rounded_preds.cpu().numpy().astype(int)) & torch.from_numpy(pre_y.cpu().numpy().astype(int))).sum()\n",
    "    recall = TP / (y.cpu().sum())\n",
    "    return recall\n",
    "\n",
    "def f1_score_avg(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    f1_score(y, rounded_preds, 'macro')\n",
    "\n",
    "# 定义训练流程\n",
    "# 把训练数据投入模型进行训练\n",
    "def train(model, iterator, optimizer, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        text = batch[0].cuda(CUDA_NUMBER)\n",
    "        label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text).squeeze(1)\n",
    "        # print(predictions)\n",
    "        loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "        acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "        recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "        label_score = label.tolist()\n",
    "\n",
    "        y_true = label_score.copy()\n",
    "\n",
    "        predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "\n",
    "        y_pred = predictions_score.copy()\n",
    "\n",
    "        F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        epoch_recall += recall.item()\n",
    "\n",
    "        epoch_f1_score += F1Score.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "# 评价\n",
    "def evaluate(model, iterator, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch[0].cuda(CUDA_NUMBER)\n",
    "            label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "            predictions = model(text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "            acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "            recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "            label_score = label.tolist()\n",
    "            y_true = label_score.copy()\n",
    "            predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "            y_pred = predictions_score.copy()\n",
    "            F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1_score += F1Score.item()\n",
    "            epoch_recall += recall.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "num_layers = 2\n",
    "print(\"num_layers\",num_layers)\n",
    "# vocab_size = 19192, embedding_dim = 100, hidden_dim = 3, output_dim = 1, dropout = 0.5, pad_idx = 1\n",
    "PAD_IDX = 1\n",
    "print('pad_idx', PAD_IDX)\n",
    "\n",
    "model = run_complex_network(word_vector_weight_matrix,\n",
    "                            EMBEDDING_DIM,\n",
    "                            HIDDEN_DIM,\n",
    "                            OUTPUT_DIM,\n",
    "                            num_layers,\n",
    "                            PAD_IDX,\n",
    "                            VOCAB_SIZE,\n",
    "                            BATCH_SIZE)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())#优化器使用的是Adam\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "best_test_acc = float('-inf')\n",
    "print(0.5 > best_test_acc)\n",
    "\n",
    "best_acc = -1\n",
    "best_f1_score = -1\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "f1_score_test = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc, train_f1_score, train_recall \\\n",
    "        = train(model, train_iterator, optimizer, criterion, CUDA_NUMBER)\n",
    "\n",
    "    test_loss, test_acc, test_f1_score, test_recall \\\n",
    "        = evaluate(model, test_iterator, criterion, CUDA_NUMBER)\n",
    "\n",
    "    acc_train.append(train_acc)\n",
    "    acc_test.append(test_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    loss_test.append(test_loss)\n",
    "    f1_score_test.append(test_f1_score)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'SST_GRU_Conv_model.pt')\n",
    "\n",
    "    # print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    # print(\n",
    "    #     f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}% | Train f1_score: {train_f1_score * 100:.2f}% | Train recall: {train_recall * 100:.2f}%')\n",
    "    # print(\n",
    "    #     f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc * 100:.2f}% | Test f1_score: {test_f1_score * 100:.2f}%| best_acc: {best_acc * 100:.2f}% | Test recall: {test_recall * 100:.2f}%')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t test  Loss: {test_loss:.3f} | test  Acc: {test_acc * 100:.2f}%')\n",
    "    print(f'\\t best  test acc: {best_test_acc * 100:.2f}%')\n",
    "\n",
    "with open('./train/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_train:\n",
    "        f.write(str(i))\n",
    "    f.close()\n",
    "\n",
    "with open('./test/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/F1_Score/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in f1_score_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/acc/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in acc_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/recall/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in test_path_ls:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "#\n",
    "colors1 = '#00CED1' \n",
    "colors2 = '#DC143C'\n",
    "colors3 = '#FFA500'\n",
    "colors4 = '#0000aFF'\n",
    "colors5 = '#FF0000'\n",
    "\n",
    "# 画初始点\n",
    "plt.figure(1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(DATA_SET + ' Loss')\n",
    "plt.plot(range(N_EPOCHS), acc_train[0:N_EPOCHS], 'r-o', label='train_loss')\n",
    "plt.plot(range(N_EPOCHS), acc_test[0:N_EPOCHS], 'c-*', label='test_loss')\n",
    "plt.xlim(-0.01, N_EPOCHS)\n",
    "plt.ylim(-0.01, 1)\n",
    "# plt.plot(range(2), sp, c=colors3)\n",
    "plt.legend()\n",
    "plt.savefig('pic' + DATA_SET + '.pdf', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num_layers 2\n",
      "pad_idx 1\n",
      "The model has 4,078,001 trainable parameters\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.558 | Train Acc: 73.31%\n",
      "\t test  Loss: 0.384 | test  Acc: 86.31%\n",
      "\t best  test acc: 86.31%\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.266 | Train Acc: 91.66%\n",
      "\t test  Loss: 0.248 | test  Acc: 91.67%\n",
      "\t best  test acc: 91.67%\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.142 | Train Acc: 96.03%\n",
      "\t test  Loss: 0.244 | test  Acc: 91.47%\n",
      "\t best  test acc: 91.67%\n",
      "Epoch: 04 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.081 | Train Acc: 98.02%\n",
      "\t test  Loss: 0.239 | test  Acc: 92.36%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.054 | Train Acc: 98.83%\n",
      "\t test  Loss: 0.285 | test  Acc: 91.27%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.032 | Train Acc: 99.40%\n",
      "\t test  Loss: 0.341 | test  Acc: 89.78%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.024 | Train Acc: 99.51%\n",
      "\t test  Loss: 0.482 | test  Acc: 88.19%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 08 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.025 | Train Acc: 99.44%\n",
      "\t test  Loss: 0.342 | test  Acc: 90.77%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.016 | Train Acc: 99.70%\n",
      "\t test  Loss: 0.373 | test  Acc: 90.58%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.010 | Train Acc: 99.84%\n",
      "\t test  Loss: 0.484 | test  Acc: 89.48%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 11 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.010 | Train Acc: 99.78%\n",
      "\t test  Loss: 0.404 | test  Acc: 90.77%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 12 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.008 | Train Acc: 99.84%\n",
      "\t test  Loss: 0.454 | test  Acc: 90.38%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.008 | Train Acc: 99.86%\n",
      "\t test  Loss: 0.426 | test  Acc: 90.28%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 14 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.007 | Train Acc: 99.85%\n",
      "\t test  Loss: 0.410 | test  Acc: 91.37%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 15 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.003 | Train Acc: 99.98%\n",
      "\t test  Loss: 0.458 | test  Acc: 90.67%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 16 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.003 | Train Acc: 99.98%\n",
      "\t test  Loss: 0.462 | test  Acc: 91.07%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 17 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.002 | Train Acc: 99.98%\n",
      "\t test  Loss: 0.481 | test  Acc: 91.07%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 18 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.003 | Train Acc: 99.98%\n",
      "\t test  Loss: 0.507 | test  Acc: 91.27%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 19 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.005 | Train Acc: 99.84%\n",
      "\t test  Loss: 0.459 | test  Acc: 89.68%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 20 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.010 | Train Acc: 99.79%\n",
      "\t test  Loss: 0.466 | test  Acc: 91.27%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 21 | Epoch Time: 0m 9s\n",
      "\t Train Loss: 0.003 | Train Acc: 99.94%\n",
      "\t test  Loss: 0.488 | test  Acc: 91.27%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 22 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.004 | Train Acc: 99.92%\n",
      "\t test  Loss: 0.492 | test  Acc: 91.27%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 23 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.003 | Train Acc: 99.95%\n",
      "\t test  Loss: 0.579 | test  Acc: 89.98%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 24 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.001 | Train Acc: 99.99%\n",
      "\t test  Loss: 0.645 | test  Acc: 90.38%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 25 | Epoch Time: 0m 10s\n",
      "\t Train Loss: 0.001 | Train Acc: 99.99%\n",
      "\t test  Loss: 0.663 | test  Acc: 90.28%\n",
      "\t best  test acc: 92.36%\n",
      "Epoch: 26 | Epoch Time: 0m 11s\n",
      "\t Train Loss: 0.001 | Train Acc: 99.99%\n",
      "\t test  Loss: 0.674 | test  Acc: 90.28%\n",
      "\t best  test acc: 92.36%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "print(torch.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DATA_SET = 'CR'\n",
    "N_EPOCHS = 26\n",
    "\n",
    "print(\"DATA_SET:\", DATA_SET)\n",
    "CUDA_NUMBER = 0\n",
    "\n",
    "# 0-->MR ||  1-->CR ||  2-->SST  ||  3-->SUBJ ||  4-->MPQA\n",
    "dataset_dict = {}\n",
    "dataset_dict['MR'] = 0\n",
    "dataset_dict['CR'] = 1\n",
    "dataset_dict['SST'] = 2\n",
    "dataset_dict['SUBJ'] = 3\n",
    "dataset_dict['MPQA'] = 4\n",
    "\n",
    "# 设置要训练的数据集\n",
    "DATASET_NUMBER = dataset_dict[DATA_SET]\n",
    "\n",
    "train_path_ls = []\n",
    "test_path_ls = []\n",
    "word_vector_path_ls = []\n",
    "train_path_ls.append(\"./DataSet/MR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/CR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SST/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SUBJ/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/MPQA/train.tsv\")\n",
    "\n",
    "test_path_ls.append(\"./DataSet/MR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/CR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SST/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SUBJ/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/MPQA/test.tsv\")\n",
    "\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/CR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SST_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SUBJ_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MPQA_word_vector.txt\")\n",
    "\n",
    "# 要训练的任务级路径\n",
    "train_path = train_path_ls[DATASET_NUMBER]\n",
    "test_path = test_path_ls[DATASET_NUMBER]\n",
    "\n",
    "# 词向量和W2Idx的路径\n",
    "word_vector_path = word_vector_path_ls[DATASET_NUMBER]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0+cu118\n",
      "DATA_SET: CR\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "source": [
    "word_dim = 100 # 词向量维度\n",
    "# 获取单词到索引的映射表以及每个单词的词向量表\n",
    "\n",
    "word_to_index = {'<unknown>': 0, '<padded>': 1}  # 根据筛选出来的词向量文件\"word_vector.txt\" 生成单词和索引的字典\n",
    "zero_ls = [0.0 for i in range(word_dim)]\n",
    "ls = [zero_ls, zero_ls]  # 用一个列表ls来存储词向量 前两个分别是 100 维的0向量，用来表示unknown_token和pad_token\n",
    "index_to_word = {}\n",
    "with open(word_vector_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # 将刑如 \"the -0.038194 -0.24487 0.72812 -0.39961...\"的字符串分成对应的词和词向量，比如\"the\"， \"-0.038194 -0.24487 0.72812 -0.39961...\"\n",
    "        #并且构建单词映射到整数的字典，word_to_index。 eg. 'the' : 2  ，PS：0和1分别是两个特殊字符串 unknown 和 padded的索引，the作为词向量文件中的第一个单词，所以下标为2\n",
    "        word_vector = line.split()\n",
    "        word_to_index[word_vector[0]] = i + 2  # 前两位由unknown和 padded分别占据\n",
    "        tmp = [] # 存储一个单词的词向量，总共是100个数字\n",
    "        for j, word in enumerate(word_vector):\n",
    "            if j == 0: #第一个是单词，所以跳过，只需要每个单词后面的词向量\n",
    "                continue\n",
    "            tmp.append(float(word))\n",
    "        ls.append(tmp) #每个单词的词向量又存到列表ls当中\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "word_vector_weight_matrix = torch.FloatTensor(ls) #将词向量列表转换为Tensor\n",
    "print(word_vector_weight_matrix.size())\n",
    "print(word_vector_weight_matrix.shape[0])\n",
    "print(len(word_to_index))#word_to_index字典的关键字是英语单词，值是单词索引，例如{'<unknown>': 0,'<padded>': 1,'the': 2,',': 3,'.': 4,...}\n",
    "VOCAB_SIZE = len(word_to_index) + 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4749, 100])\n",
      "4749\n",
      "4749\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# 导入AFINN情感词典\n",
    "# from afinn import Afinn\n",
    "\n",
    "# 创建Afinn对象\n",
    "# afinn = Afinn()\n",
    "\n",
    "# 1、【将句子分词，并且求句子长度】\n",
    "def get_sentences(path):\n",
    "    sentences = []\n",
    "    batch_sentences = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # 跳过第一行的test label\n",
    "            if i == 0:\n",
    "                continue\n",
    "            words = line.split()[:-1]  # 需要将末尾的0或者1去掉\n",
    "            label = float(line.split()[-1])\n",
    "            count = 0  # 统计每个句子的长度\n",
    "            for word in words:\n",
    "                count += 1\n",
    "            tmp = (words, count, label)\n",
    "            sentences.append(tmp)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# 根据上面得到的word_to_index,将单词转换为数字,列表形式为[ ([words_list1], len1, label1), ([words_list2], len2, label2),......],\n",
    "# word_list是多个单词组成的序列\n",
    "\n",
    "# 2、【将分词之后的单词序列变成数字序列】\n",
    "def lookup_table(array_ls, word_to_index):\n",
    "    sentences_tensor = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tensor_ls = []\n",
    "        # sentence[0]是一个包含多个单词的列表。sentence[0]是一个列表，里面的元素是单词。\n",
    "        for word in sentence[0]:\n",
    "            # word_to_index 是一个单词到索引的字典\n",
    "            if word in word_to_index.keys():\n",
    "                tensor_ls.append(word_to_index[word])  # 将单词转换为索引，并且索引存入张量当中\n",
    "            else:\n",
    "                tensor_ls.append(1)  # 如果在索引表中没找到单词，则“不认识”单词,用1下标代替, 此时unknown_token和padded_token下标都是1\n",
    "        sentences_tensor.append((tensor_ls, sentence[1], sentence[2]))\n",
    "    return sentences_tensor  # 得到的列表形式为[([3,8,12,123,2,9],6,1),([6,45,652345,634,2342,4],6,0)]\n",
    "\n",
    "\n",
    "# 对句子进行填充，eg:16个句子组成一个batch,【每个batch的句子长度必须相等】，这里的做法是，获取batch中长度最长的句子,然后句子长度不够的append 1\n",
    "# 输入 [([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. [([1, 2, 3],3, 0), ([2, 3], 2, 1), ...]\n",
    "# 输出，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. 假设batch_size = 2\n",
    "# [([1, 2, 3], 3), ([2, 3, 1], 2),...] 第二个句子列表 append 1，但是实际长度为2\n",
    "\n",
    "# 3、【将句子进行分batch，并且将数字列表（句子）进行填充】\n",
    "def pad_sentence_plus(array_ls, array_sentiment, batch_size):  # 这里的array_ls是上面函数的输出，即sentences_tensor。\n",
    "    ans = 0  # j记录每个batch的第一条数据的下标\n",
    "    max = array_ls[0][1]  # max为每个batch的句子最大长度.\n",
    "    # 第一维为列表的第几个元素（即一个句子元组），第二维为一个元组的第一个元素（即一个元组里面的word_list），这里max初始化为第一个句子的长度。\n",
    "    for i in range(len(array_ls)):\n",
    "        # 需要考虑最后一个batch可能长度不够batch_size\n",
    "        if (i + 1) % batch_size == 0 or i == len(array_ls) - 1:\n",
    "            if array_ls[i][1] > max:\n",
    "                max = array_ls[i][1]\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                index = batch_size\n",
    "            else:\n",
    "                index = i - ans + 1\n",
    "            for j in range(index):\n",
    "                while len(array_ls[j + ans][0]) < max:\n",
    "                    array_ls[j + ans][0].append(1)\n",
    "                    array_sentiment[j + ans] += [0]\n",
    "            # 每一次填充完毕后，需要更新标记，并再次初始化最大值\n",
    "            ans = i + 1\n",
    "            if ans != len(array_ls):\n",
    "                max = array_ls[ans][1]\n",
    "        else:\n",
    "            if array_ls[i][1] > max:  # 取句子的最大长度\n",
    "                max = array_ls[i][1]\n",
    "    return array_ls, array_sentiment\n",
    "\n",
    "\n",
    "# 输入，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# [([1, 2, 3],3), ([2, 3, 1], 2),...]\n",
    "# shuffle表示是否将每个batch打乱\n",
    "# batch_first if false 表示返回的张量形状为(sentence_len,batch_size)\n",
    "# if true 表示返回的张量形状为(batch_size,sentence_len)\n",
    "# 这里默认采用batch_first = False,主要是为了适应nn.Embedding层的输入形状\n",
    "\n",
    "# 4、【获得embedding的一个个batch（调整batch的的形状）的数据迭代器】\n",
    "def iterator(array_ls, array_sentiment, batch_size, shuffle=True, batch_first=False):\n",
    "    sentences_index_label_sentiment = []  # 存放填充后的语句列表【句子的indx表示】\n",
    "    tmp_sen = []\n",
    "    tmp_label = []\n",
    "    tmp_sentiment = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tmp_sen.append(sentence[0])  # 存放一个batch的【数据】\n",
    "        tmp_label.append(sentence[2])  # 存放一个batch的【标签】\n",
    "        tmp_sentiment.append(array_sentiment[i])\n",
    "        if (i + 1) % batch_size == 0:  #\n",
    "            sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "            tmp_sen = []  # 清空数据\n",
    "            tmp_label = []  # 清空标签\n",
    "            tmp_sentiment = []\n",
    "\n",
    "    # 最后几个样本可能不够一个batch,需要额外判断\n",
    "    if len(tmp_sen) != 0:\n",
    "        # # 补齐\n",
    "        # for _ in range(batch_size - len(tmp_sen)):\n",
    "        #     tmp_sen.append(sentence[0])\n",
    "        #     tmp_label.append(sentence[2])\n",
    "        #     tmp_sentiment.append(array_sentiment[1])\n",
    "        sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "        # # OR去掉\n",
    "        # break\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(sentences_index_label_sentiment)  # 打乱列表中各个batch的顺序\n",
    "    res = []\n",
    "    # 2D张量转置\n",
    "    # 2D张量转置\n",
    "    if batch_first == False:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]),\n",
    "                        torch.FloatTensor(batch[2]).t()))  # 函数t()求矩阵的转置\n",
    "    else:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]), torch.FloatTensor(batch[2]).t()))\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet, sentiwordnet\n",
    "# 下载 SentiWordNet 数据\n",
    "# nltk.download('sentiwordnet')\n",
    "# 加载 SentiWordNet 数据\n",
    "swn_file_path = nltk.data.find('corpora/sentiwordnet/SentiWordNet_3.0.0.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "source": [
    "# =====================================================================================\n",
    "# ================================= sentiwordnet ======================================\n",
    "# =====================================================================================\n",
    "\n",
    "\n",
    "def get_sentiment_score(word):\n",
    "    \"\"\"获取WordNet情感分数\"\"\"\n",
    "\n",
    "    synsets = list(wordnet.synsets(word))\n",
    "    if synsets:\n",
    "        sentiment_scores = [\n",
    "            sentiwordnet.senti_synset(s.name()).pos_score() - sentiwordnet.senti_synset(s.name()).neg_score()\n",
    "            for s in synsets\n",
    "        ]\n",
    "        avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "        return avg_sentiment_score\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def get_word_sentiment_scores(tokenized_sentence):\n",
    "    \"\"\"计算已分词句子中每个单词的情感极性分数\"\"\"\n",
    "    # 处理否定词\n",
    "    word_scores = [get_sentiment_score(word) for word in tokenized_sentence]\n",
    "    return word_scores\n",
    "\n",
    "\n",
    "def get_batch_word_sentiment_scores(batch_tokenized_sentences):\n",
    "    \"\"\"处理一批已分词的文本并返回其中每个单词的情感极性分数\"\"\"\n",
    "    batch_word_scores = []\n",
    "    for sentence, _, _ in batch_tokenized_sentences:\n",
    "        batch_word_scores.append(get_word_sentiment_scores(sentence))\n",
    "    return batch_word_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED) # 为CPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.cuda.manual_seed(SEED) # 为GPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_NUMBER}' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "source": [
    "def get_iterator(path, batch_size, word_to_index):\n",
    "    # ['another', 'thing', 'i', 'don', \"'t\", 'like', 'is', 'that', 'some', 'of', 'the',\n",
    "    # 'features', 'take', '1-2', 'seconds', 'to', 'load', 'up', '.']\n",
    "    sentences = get_sentences(path)\n",
    "    # ([2, 3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
    "    # 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 19, 0.0)\n",
    "    # 将单词变成indx\n",
    "    sentences_indx = lookup_table(sentences, word_to_index)\n",
    "\n",
    "    # get sentiment polarity by 【AFINN】\n",
    "    # afinn = Afinn()\n",
    "    # sentiment_sentences_AFINN = get_batch_word_sentiment_scores_AFINN(sentences,afinn)\n",
    "    # sentiment_sentences_AFINN = calculate_sentiment_for_phrases(sentences, afinn, n=3)\n",
    "\n",
    "    # get sentiment polarity by 【sentiwordnet】\n",
    "    sentiment_sentences_sentiwordnet = get_batch_word_sentiment_scores(sentences)\n",
    "    sentences_padded, sentiment_sentences_padded = pad_sentence_plus(sentences_indx, sentiment_sentences_sentiwordnet,\n",
    "                                                                     batch_size)\n",
    "\n",
    "    # 把分成一个个batch，然后每个list之内有数据和标签\n",
    "    Iterator_with_sentiments = iterator(sentences_padded, sentiment_sentences_padded, batch_size)\n",
    "\n",
    "    return Iterator_with_sentiments\n",
    "\n",
    "# 2、生成对应任务的句子的词向量\n",
    "train_iterator = get_iterator(train_path, BATCH_SIZE, word_to_index)\n",
    "test_iterator = get_iterator(test_path, BATCH_SIZE, word_to_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "source": [
    "class projection_Euler(nn.Module):\n",
    "    def __init__(self, Embedding, num_layers):\n",
    "        super(projection_Euler, self).__init__()  # projection是子类名\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        amplitude = inputs[0]\n",
    "        phase = inputs[1]\n",
    "        # ================ 变换形状和归一化处理 ================\n",
    "        # -----\n",
    "        amplitude_permute = amplitude.permute(1, 0, 2)\n",
    "        amplitude_norm = F.normalize(amplitude_permute, 2, 2)\n",
    "        # ----\n",
    "        phase_permute = phase.permute(1, 0, 2)\n",
    "\n",
    "        # ================ 计算Am和Ph分别计算实部和虚部 ================\n",
    "        # --- 求欧拉展开式\n",
    "        real_part = amplitude_norm * torch.cos(phase_permute)\n",
    "        imag_part = amplitude_norm * torch.sin(phase_permute)\n",
    "\n",
    "        return [real_part, imag_part]\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixMean(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixMean, self).__init__()\n",
    "        self.mapping_Diagonal_q = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.mapping_Diagonal_k = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.embed_size = Embedding_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        Diagonal1 = inputs[0]\n",
    "        Diagonal2 = inputs[1]\n",
    "        Matrix = inputs[2]\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attention_scores = Diagonal1 * Diagonal2\n",
    "        # 迹求和\n",
    "        attention_weight = torch.sum(attention_scores, dim=-1)\n",
    "\n",
    "        # 归一化\n",
    "        attention = F.softmax(attention_weight, dim=-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # 加权求和\n",
    "        Matrix_output = torch.sum(Matrix * attention, dim=1)\n",
    "\n",
    "        return Matrix_output\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixQ(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixQ, self).__init__()\n",
    "        self.Q_Linear_real1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.Q_Linear_real2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.AttentionReal = projection_CalculateMatrixMean(Embedding_dim)\n",
    "        self.AttentionImag = projection_CalculateMatrixMean(Embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_real = inputs[0]  # 实部\n",
    "        input_imag = inputs[1]  # 虚部\n",
    "        # ==================== 1 计算对角线和密度矩阵 ====================\n",
    "\n",
    "        # ======= 1.1 计算对角线 =======\n",
    "        input_real1 = self.Q_Linear_real1(input_real)\n",
    "        input_imag1 = self.Q_Linear_imag1(input_imag)\n",
    "\n",
    "        # 获得[实部/虚部]对角线向量\n",
    "        diagonal_real1 = input_real1 * input_real1 - input_imag1 * input_imag1\n",
    "        diagonal_imag1 = input_real1 * input_imag1 + input_imag1 * input_real1\n",
    "\n",
    "        # ======= 1.2 计算密度矩阵 =======\n",
    "        input_real2 = self.Q_Linear_real2(input_real)\n",
    "        input_imag2 = self.Q_Linear_imag2(input_imag)\n",
    "\n",
    "        diagonal_real2 = input_real2 * input_real2 - input_imag2 * input_imag2\n",
    "        diagonal_imag2 = input_real2 * input_imag2 + input_imag2 * input_real2\n",
    "\n",
    "        real_part_expand = torch.unsqueeze(input_real2, dim=3)\n",
    "        imag_part_expand = torch.unsqueeze(input_imag2, dim=3)\n",
    "\n",
    "        real_part_expand_transpose = real_part_expand.permute(0, 1, 3, 2)\n",
    "        imag_part_expand_transpose = imag_part_expand.permute(0, 1, 3, 2)\n",
    "\n",
    "        # real 密度矩阵\n",
    "        Matrix_real = torch.matmul(real_part_expand, real_part_expand_transpose) - torch.matmul(imag_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "        # imag 密度矩阵\n",
    "        Matrix_imag = torch.matmul(imag_part_expand, real_part_expand_transpose) + torch.matmul(real_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "\n",
    "        Matrix_Real_output = self.AttentionReal([diagonal_real1, diagonal_real2, Matrix_real])\n",
    "        Matrix_imag_output = self.AttentionImag([diagonal_imag1, diagonal_imag2, Matrix_imag])\n",
    "\n",
    "        return [Matrix_Real_output, Matrix_imag_output]\n",
    "\n",
    "\n",
    "class projection_Measurement(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_Measurement, self).__init__()  # projection是子类名\n",
    "        # 使得tensor是正交的,正交初始化,projector是子类projection的新属性。\n",
    "        self.projector = nn.init.orthogonal_(Parameter(torch.Tensor(2, Embedding_dim, 1)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        v_real_avg = inputs[0]\n",
    "        v_imag_avg = inputs[1]\n",
    "\n",
    "        p_real = self.projector[0]\n",
    "        p_imag = self.projector[1]\n",
    "\n",
    "        p_real_norm = p_real / torch.norm(p_real, dim=0)\n",
    "        p_imag_norm = p_imag / torch.norm(p_imag, dim=0)\n",
    "\n",
    "        p_real_mat = torch.matmul(p_real_norm, p_real_norm.permute(1, 0))\n",
    "        p_imag_mat = torch.matmul(p_imag_norm, p_imag_norm.permute(1, 0))\n",
    "\n",
    "        Pv_real = torch.matmul(v_real_avg, p_real_mat) - torch.matmul(v_imag_avg, p_imag_mat)\n",
    "        Pv_imag = torch.matmul(v_real_avg, p_imag_mat) + torch.matmul(v_imag_avg, p_real_mat)\n",
    "\n",
    "        Pv_real_plus = torch.unsqueeze(Pv_real, dim=1)\n",
    "        Pv_imag_plus = torch.unsqueeze(Pv_imag, dim=1)\n",
    "\n",
    "        return [Pv_real_plus, Pv_imag_plus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "source": [
    "# 映射成三个不同的向量的自注意力机制\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.mapping_query = nn.init.uniform_(\n",
    "            Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))  # 从均匀分布U(a, b)中生成值（随机生成），填充输入的张量或变量\n",
    "        self.mapping_key = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "        self.mapping_value = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = torch.matmul(inputs.permute(1, 0, 2), self.mapping_query)  # permute函数为换维函数，参数为(0，1，2)\n",
    "        key = torch.matmul(inputs.permute(1, 0, 2), self.mapping_key)  # matmul是tensor矩阵乘法\n",
    "        value = torch.matmul(inputs.permute(1, 0, 2), self.mapping_value)\n",
    "\n",
    "        query = query / torch.norm(query, dim=2).reshape(query.shape[0], query.shape[1],\n",
    "                                                         1)  # query.shape[0]=16,query.shape[1]=sentence_length.\n",
    "        key = key / torch.norm(key, dim=2).reshape(key.shape[0], key.shape[1],\n",
    "                                                   1)  # shape of tensor:[16,sentence_length,1]\n",
    "\n",
    "        value = value / torch.norm(value, dim=2).reshape(value.shape[0], value.shape[1], 1)  # query,key,value的形状是一样的\n",
    "\n",
    "        xx = torch.matmul(query, torch.transpose(key, 1, 2)) / np.sqrt(inputs.shape[1])\n",
    "        xx1 = torch.softmax(xx, dim=2)\n",
    "        out = torch.matmul(xx1, value)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "source": [
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, Embedding_dim, num_layers):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "\n",
    "        # GRU layer\n",
    "        self.rnn = nn.GRU(Embedding_dim, Embedding_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "        # self.scale_layer = nn.Linear(2 * Embedding_dim, Embedding_dim)\n",
    "\n",
    "        # Linear layer for skip connection\n",
    "        self.middle_layer = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through GRU layer\n",
    "        output, _ = self.rnn(x)\n",
    "\n",
    "        # Skip connection: Add output of middle layer to GRU layer output\n",
    "        output = output + self.middle_layer(x)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "source": [
    "# -------------------- 01 Model_best_copy8672_cnn_maxpooling ------------------------\n",
    "print(\"Model_best_copy8672_cnn_maxpooling\")\n",
    "class run_complex_network(nn.Module):\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, output_dim, num_layers, pad_idx, VOCAB_SIZE, batch):\n",
    "        super().__init__()\n",
    "\n",
    "        # ================ 1、词嵌入模块 ================\n",
    "        # ------ 实部，预训练模型 -------\n",
    "        self.amplitude_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "\n",
    "        # self.amplitude_embedding2 = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ------ 虚部，传统模型 -------\n",
    "        self.phase_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "        # self.phase_embedding = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "        # self.phase_embedding_sentiment = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ================ 2、GRU模块 信息提取 ================\n",
    "        # 输出张量的维度(L:sequence_length,N:batch_size,hidden_size)\n",
    "        self.gru1 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "        self.gru2 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "\n",
    "        self.self_attention1 = self_attention(Embedding_dim=embedding_dim)\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "\n",
    "        self.projection_Euler = projection_Euler(Embedding=embedding_dim, num_layers=num_layers)\n",
    "        self.projection_CalculateMatrixattension = projection_CalculateMatrixQ(Embedding_dim=embedding_dim)\n",
    "        self.projection_measurement = projection_Measurement(embedding_dim)\n",
    "\n",
    "        # ================ 4、卷积 ================\n",
    "        self.Conv2dOne = nn.Conv2d(1, 1, 3)  # 实部\n",
    "        self.Conv2dTwo = nn.Conv2d(1, 1, 3)  # 虚部\n",
    "\n",
    "        # ================ 5、池化 ================\n",
    "\n",
    "        self.MaxPool1 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 实部\n",
    "        self.MaxPool2 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 虚部\n",
    "\n",
    "        # ================ 6、全连接层 ================\n",
    "        self.fc1 = nn.Linear(2 * (embedding_dim - 2), 10)\n",
    "        self.fc2 = nn.Linear(10, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # ================ 1、文本嵌入到向量 ================\n",
    "        amplitude_is_WordEmbedding = self.amplitude_embedding(text)\n",
    "\n",
    "        phase_is_sentiment = self.phase_embedding(text)\n",
    "\n",
    "        # ================ 2、实部进行GRU处理 ================\n",
    "        amplitude_plus = self.gru1(amplitude_is_WordEmbedding)\n",
    "        amplitude_plus2 = self.self_attention1(amplitude_plus)\n",
    "        amplitude_plus2 = amplitude_plus2.permute(1, 0, 2)\n",
    "\n",
    "        phase_plus = self.gru2(phase_is_sentiment)\n",
    "\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "        embedded = [amplitude_plus2, phase_plus]\n",
    "\n",
    "        Euler_realAimag = self.projection_Euler(embedded)\n",
    "        Matrix_realAimag = self.projection_CalculateMatrixattension(Euler_realAimag)\n",
    "        Project_realAimag = self.projection_measurement(Matrix_realAimag)\n",
    "\n",
    "        MatrixReal = Project_realAimag[0]\n",
    "        MatrixImag = Project_realAimag[1]\n",
    "\n",
    "        # ================ 4、卷积操作 ================\n",
    "        # 5.1 --对于【实部】进行卷积\n",
    "        Conv_real = self.Conv2dOne(MatrixReal)\n",
    "        # 5.2 --对于【虚部】进行卷积\n",
    "        Conv_imag = self.Conv2dTwo(MatrixImag)\n",
    "\n",
    "        # ================ 5、池化操作 ================\n",
    "        # 6.1 --对于【实部】进行池化\n",
    "        Max_real = self.MaxPool1(torch.sigmoid(Conv_real))\n",
    "        # 6.2 --对于【虚部】进行池化\n",
    "        Max_imag = self.MaxPool2(torch.sigmoid(Conv_imag))\n",
    "\n",
    "        # ================ 6、全连接 ================\n",
    "        fc1 = self.fc1(torch.cat((Max_real, Max_imag), dim=3))\n",
    "        fc2 = torch.sigmoid(self.fc2(torch.sigmoid(fc1)))\n",
    "\n",
    "        return fc2  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_best_copy8672_cnn_maxpooling\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def multiply(input1, input2):\n",
    "    return torch.sum(torch.mul(input1, input2)).item()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    # print('batch: 模型预测值', rounded_preds,'真实标签', y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def calculation_recall(preds, y):\n",
    "    pre = preds.detach()\n",
    "    pre_y = y.detach()\n",
    "    rounded_preds = torch.round(pre)\n",
    "    \n",
    "    TP = (torch.from_numpy(rounded_preds.cpu().numpy().astype(int)) & torch.from_numpy(pre_y.cpu().numpy().astype(int))).sum()\n",
    "    recall = TP / (y.cpu().sum())\n",
    "    return recall\n",
    "\n",
    "def f1_score_avg(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    f1_score(y, rounded_preds, 'macro')\n",
    "\n",
    "# 定义训练流程\n",
    "# 把训练数据投入模型进行训练\n",
    "def train(model, iterator, optimizer, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        text = batch[0].cuda(CUDA_NUMBER)\n",
    "        label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text).squeeze(1)\n",
    "        # print(predictions)\n",
    "        loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "        acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "        recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "        label_score = label.tolist()\n",
    "\n",
    "        y_true = label_score.copy()\n",
    "\n",
    "        predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "\n",
    "        y_pred = predictions_score.copy()\n",
    "\n",
    "        F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        epoch_recall += recall.item()\n",
    "\n",
    "        epoch_f1_score += F1Score.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "# 评价\n",
    "def evaluate(model, iterator, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch[0].cuda(CUDA_NUMBER)\n",
    "            label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "            predictions = model(text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "            acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "            recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "            label_score = label.tolist()\n",
    "            y_true = label_score.copy()\n",
    "            predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "            y_pred = predictions_score.copy()\n",
    "            F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1_score += F1Score.item()\n",
    "            epoch_recall += recall.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "num_layers = 2\n",
    "print(\"num_layers\",num_layers)\n",
    "# vocab_size = 19192, embedding_dim = 100, hidden_dim = 3, output_dim = 1, dropout = 0.5, pad_idx = 1\n",
    "PAD_IDX = 1\n",
    "print('pad_idx', PAD_IDX)\n",
    "\n",
    "model = run_complex_network(word_vector_weight_matrix,\n",
    "                            EMBEDDING_DIM,\n",
    "                            HIDDEN_DIM,\n",
    "                            OUTPUT_DIM,\n",
    "                            num_layers,\n",
    "                            PAD_IDX,\n",
    "                            VOCAB_SIZE,\n",
    "                            BATCH_SIZE)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())#优化器使用的是Adam\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "best_test_acc = float('-inf')\n",
    "print(0.5 > best_test_acc)\n",
    "\n",
    "best_acc = -1\n",
    "best_f1_score = -1\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "f1_score_test = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc, train_f1_score, train_recall \\\n",
    "        = train(model, train_iterator, optimizer, criterion, CUDA_NUMBER)\n",
    "\n",
    "    test_loss, test_acc, test_f1_score, test_recall \\\n",
    "        = evaluate(model, test_iterator, criterion, CUDA_NUMBER)\n",
    "\n",
    "    acc_train.append(train_acc)\n",
    "    acc_test.append(test_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    loss_test.append(test_loss)\n",
    "    f1_score_test.append(test_f1_score)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'SST_GRU_Conv_model.pt')\n",
    "\n",
    "    # print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    # print(\n",
    "    #     f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}% | Train f1_score: {train_f1_score * 100:.2f}% | Train recall: {train_recall * 100:.2f}%')\n",
    "    # print(\n",
    "    #     f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc * 100:.2f}% | Test f1_score: {test_f1_score * 100:.2f}%| best_acc: {best_acc * 100:.2f}% | Test recall: {test_recall * 100:.2f}%')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t test  Loss: {test_loss:.3f} | test  Acc: {test_acc * 100:.2f}%')\n",
    "    print(f'\\t best  test acc: {best_test_acc * 100:.2f}%')\n",
    "\n",
    "with open('./train/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_train:\n",
    "        f.write(str(i))\n",
    "    f.close()\n",
    "\n",
    "with open('./test/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/F1_Score/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in f1_score_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/acc/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in acc_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/recall/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in test_path_ls:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "#\n",
    "colors1 = '#00CED1' \n",
    "colors2 = '#DC143C'\n",
    "colors3 = '#FFA500'\n",
    "colors4 = '#0000aFF'\n",
    "colors5 = '#FF0000'\n",
    "\n",
    "# 画初始点\n",
    "plt.figure(1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(DATA_SET + ' Loss')\n",
    "plt.plot(range(N_EPOCHS), acc_train[0:N_EPOCHS], 'r-o', label='train_loss')\n",
    "plt.plot(range(N_EPOCHS), acc_test[0:N_EPOCHS], 'c-*', label='test_loss')\n",
    "plt.xlim(-0.01, N_EPOCHS)\n",
    "plt.ylim(-0.01, 1)\n",
    "# plt.plot(range(2), sp, c=colors3)\n",
    "plt.legend()\n",
    "plt.savefig('pic' + DATA_SET + '.pdf', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num_layers 2\n",
      "pad_idx 1\n",
      "The model has 1,325,401 trainable parameters\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.657 | Train Acc: 63.62%\n",
      "\t test  Loss: 0.653 | test  Acc: 64.06%\n",
      "\t best  test acc: 64.06%\n",
      "Epoch: 02 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.643 | Train Acc: 63.62%\n",
      "\t test  Loss: 0.606 | test  Acc: 64.06%\n",
      "\t best  test acc: 64.06%\n",
      "Epoch: 03 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.512 | Train Acc: 79.13%\n",
      "\t test  Loss: 0.472 | test  Acc: 81.51%\n",
      "\t best  test acc: 81.51%\n",
      "Epoch: 04 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.433 | Train Acc: 83.17%\n",
      "\t test  Loss: 0.438 | test  Acc: 82.29%\n",
      "\t best  test acc: 82.29%\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.374 | Train Acc: 86.24%\n",
      "\t test  Loss: 0.472 | test  Acc: 81.25%\n",
      "\t best  test acc: 82.29%\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.363 | Train Acc: 86.87%\n",
      "\t test  Loss: 0.467 | test  Acc: 82.29%\n",
      "\t best  test acc: 82.29%\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.297 | Train Acc: 90.28%\n",
      "\t test  Loss: 0.437 | test  Acc: 82.81%\n",
      "\t best  test acc: 82.81%\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.268 | Train Acc: 90.71%\n",
      "\t test  Loss: 0.446 | test  Acc: 83.33%\n",
      "\t best  test acc: 83.33%\n",
      "Epoch: 09 | Epoch Time: 0m 2s\n",
      "\t Train Loss: 0.249 | Train Acc: 92.20%\n",
      "\t test  Loss: 0.521 | test  Acc: 79.17%\n",
      "\t best  test acc: 83.33%\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.226 | Train Acc: 93.09%\n",
      "\t test  Loss: 0.444 | test  Acc: 84.90%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 11 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.185 | Train Acc: 94.51%\n",
      "\t test  Loss: 0.445 | test  Acc: 84.90%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 12 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.177 | Train Acc: 94.74%\n",
      "\t test  Loss: 0.537 | test  Acc: 82.55%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 13 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.146 | Train Acc: 96.00%\n",
      "\t test  Loss: 0.496 | test  Acc: 83.59%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 14 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.140 | Train Acc: 96.20%\n",
      "\t test  Loss: 0.506 | test  Acc: 84.38%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 15 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.115 | Train Acc: 96.96%\n",
      "\t test  Loss: 0.500 | test  Acc: 84.90%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 16 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.104 | Train Acc: 97.35%\n",
      "\t test  Loss: 0.543 | test  Acc: 84.11%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 17 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.137 | Train Acc: 95.90%\n",
      "\t test  Loss: 0.583 | test  Acc: 81.25%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 18 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.135 | Train Acc: 95.67%\n",
      "\t test  Loss: 0.550 | test  Acc: 83.85%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 19 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.100 | Train Acc: 96.69%\n",
      "\t test  Loss: 0.575 | test  Acc: 82.81%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 20 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.090 | Train Acc: 96.89%\n",
      "\t test  Loss: 0.626 | test  Acc: 83.59%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 21 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.088 | Train Acc: 96.89%\n",
      "\t test  Loss: 0.545 | test  Acc: 84.38%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 22 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.142 | Train Acc: 94.54%\n",
      "\t test  Loss: 0.640 | test  Acc: 82.03%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 23 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.110 | Train Acc: 95.97%\n",
      "\t test  Loss: 0.550 | test  Acc: 84.11%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 24 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.073 | Train Acc: 97.75%\n",
      "\t test  Loss: 0.587 | test  Acc: 83.33%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 25 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.063 | Train Acc: 97.95%\n",
      "\t test  Loss: 0.662 | test  Acc: 81.77%\n",
      "\t best  test acc: 84.90%\n",
      "Epoch: 26 | Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.078 | Train Acc: 97.59%\n",
      "\t test  Loss: 0.684 | test  Acc: 81.25%\n",
      "\t best  test acc: 84.90%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "print(torch.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DATA_SET = 'SST'\n",
    "N_EPOCHS = 26\n",
    "\n",
    "print(\"DATA_SET:\", DATA_SET)\n",
    "CUDA_NUMBER = 0\n",
    "\n",
    "# 0-->MR ||  1-->CR ||  2-->SST  ||  3-->SUBJ ||  4-->MPQA\n",
    "dataset_dict = {}\n",
    "dataset_dict['MR'] = 0\n",
    "dataset_dict['CR'] = 1\n",
    "dataset_dict['SST'] = 2\n",
    "dataset_dict['SUBJ'] = 3\n",
    "dataset_dict['MPQA'] = 4\n",
    "\n",
    "# 设置要训练的数据集\n",
    "DATASET_NUMBER = dataset_dict[DATA_SET]\n",
    "\n",
    "train_path_ls = []\n",
    "test_path_ls = []\n",
    "word_vector_path_ls = []\n",
    "train_path_ls.append(\"./DataSet/MR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/CR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SST/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SUBJ/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/MPQA/train.tsv\")\n",
    "\n",
    "test_path_ls.append(\"./DataSet/MR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/CR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SST/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SUBJ/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/MPQA/test.tsv\")\n",
    "\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/CR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SST_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SUBJ_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MPQA_word_vector.txt\")\n",
    "\n",
    "# 要训练的任务级路径\n",
    "train_path = train_path_ls[DATASET_NUMBER]\n",
    "test_path = test_path_ls[DATASET_NUMBER]\n",
    "\n",
    "# 词向量和W2Idx的路径\n",
    "word_vector_path = word_vector_path_ls[DATASET_NUMBER]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0+cu118\n",
      "DATA_SET: SST\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "print(torch.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DATA_SET = 'SST'\n",
    "N_EPOCHS = 16\n",
    "\n",
    "print(\"DATA_SET:\", DATA_SET)\n",
    "CUDA_NUMBER = 0\n",
    "\n",
    "# 0-->MR ||  1-->CR ||  2-->SST  ||  3-->SUBJ ||  4-->MPQA\n",
    "dataset_dict = {}\n",
    "dataset_dict['MR'] = 0\n",
    "dataset_dict['CR'] = 1\n",
    "dataset_dict['SST'] = 2\n",
    "dataset_dict['SUBJ'] = 3\n",
    "dataset_dict['MPQA'] = 4\n",
    "\n",
    "# 设置要训练的数据集\n",
    "DATASET_NUMBER = dataset_dict[DATA_SET]\n",
    "\n",
    "train_path_ls = []\n",
    "test_path_ls = []\n",
    "word_vector_path_ls = []\n",
    "train_path_ls.append(\"./DataSet/MR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/CR/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SST/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/SUBJ/train.tsv\")\n",
    "train_path_ls.append(\"./DataSet/MPQA/train.tsv\")\n",
    "\n",
    "test_path_ls.append(\"./DataSet/MR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/CR/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SST/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/SUBJ/test.tsv\")\n",
    "test_path_ls.append(\"./DataSet/MPQA/test.tsv\")\n",
    "\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/CR_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SST_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/SUBJ_word_vector.txt\")\n",
    "word_vector_path_ls.append(\"./sub_word_vector/MPQA_word_vector.txt\")\n",
    "\n",
    "# 要训练的任务级路径\n",
    "train_path = train_path_ls[DATASET_NUMBER]\n",
    "test_path = test_path_ls[DATASET_NUMBER]\n",
    "\n",
    "# 词向量和W2Idx的路径\n",
    "word_vector_path = word_vector_path_ls[DATASET_NUMBER]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0+cu118\n",
      "DATA_SET: SST\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "source": [
    "word_dim = 100 # 词向量维度\n",
    "# 获取单词到索引的映射表以及每个单词的词向量表\n",
    "\n",
    "word_to_index = {'<unknown>': 0, '<padded>': 1}  # 根据筛选出来的词向量文件\"word_vector.txt\" 生成单词和索引的字典\n",
    "zero_ls = [0.0 for i in range(word_dim)]\n",
    "ls = [zero_ls, zero_ls]  # 用一个列表ls来存储词向量 前两个分别是 100 维的0向量，用来表示unknown_token和pad_token\n",
    "index_to_word = {}\n",
    "with open(word_vector_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # 将刑如 \"the -0.038194 -0.24487 0.72812 -0.39961...\"的字符串分成对应的词和词向量，比如\"the\"， \"-0.038194 -0.24487 0.72812 -0.39961...\"\n",
    "        #并且构建单词映射到整数的字典，word_to_index。 eg. 'the' : 2  ，PS：0和1分别是两个特殊字符串 unknown 和 padded的索引，the作为词向量文件中的第一个单词，所以下标为2\n",
    "        word_vector = line.split()\n",
    "        word_to_index[word_vector[0]] = i + 2  # 前两位由unknown和 padded分别占据\n",
    "        tmp = [] # 存储一个单词的词向量，总共是100个数字\n",
    "        for j, word in enumerate(word_vector):\n",
    "            if j == 0: #第一个是单词，所以跳过，只需要每个单词后面的词向量\n",
    "                continue\n",
    "            tmp.append(float(word))\n",
    "        ls.append(tmp) #每个单词的词向量又存到列表ls当中\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "word_vector_weight_matrix = torch.FloatTensor(ls) #将词向量列表转换为Tensor\n",
    "print(word_vector_weight_matrix.size())\n",
    "print(word_vector_weight_matrix.shape[0])\n",
    "print(len(word_to_index))#word_to_index字典的关键字是英语单词，值是单词索引，例如{'<unknown>': 0,'<padded>': 1,'the': 2,',': 3,'.': 4,...}\n",
    "VOCAB_SIZE = len(word_to_index) + 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([13772, 100])\n",
      "13772\n",
      "13772\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# 导入AFINN情感词典\n",
    "# from afinn import Afinn\n",
    "\n",
    "# 创建Afinn对象\n",
    "# afinn = Afinn()\n",
    "\n",
    "# 1、【将句子分词，并且求句子长度】\n",
    "def get_sentences(path):\n",
    "    sentences = []\n",
    "    batch_sentences = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # 跳过第一行的test label\n",
    "            if i == 0:\n",
    "                continue\n",
    "            words = line.split()[:-1]  # 需要将末尾的0或者1去掉\n",
    "            label = float(line.split()[-1])\n",
    "            count = 0  # 统计每个句子的长度\n",
    "            for word in words:\n",
    "                count += 1\n",
    "            tmp = (words, count, label)\n",
    "            sentences.append(tmp)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# 根据上面得到的word_to_index,将单词转换为数字,列表形式为[ ([words_list1], len1, label1), ([words_list2], len2, label2),......],\n",
    "# word_list是多个单词组成的序列\n",
    "\n",
    "# 2、【将分词之后的单词序列变成数字序列】\n",
    "def lookup_table(array_ls, word_to_index):\n",
    "    sentences_tensor = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tensor_ls = []\n",
    "        # sentence[0]是一个包含多个单词的列表。sentence[0]是一个列表，里面的元素是单词。\n",
    "        for word in sentence[0]:\n",
    "            # word_to_index 是一个单词到索引的字典\n",
    "            if word in word_to_index.keys():\n",
    "                tensor_ls.append(word_to_index[word])  # 将单词转换为索引，并且索引存入张量当中\n",
    "            else:\n",
    "                tensor_ls.append(1)  # 如果在索引表中没找到单词，则“不认识”单词,用1下标代替, 此时unknown_token和padded_token下标都是1\n",
    "        sentences_tensor.append((tensor_ls, sentence[1], sentence[2]))\n",
    "    return sentences_tensor  # 得到的列表形式为[([3,8,12,123,2,9],6,1),([6,45,652345,634,2342,4],6,0)]\n",
    "\n",
    "\n",
    "# 对句子进行填充，eg:16个句子组成一个batch,【每个batch的句子长度必须相等】，这里的做法是，获取batch中长度最长的句子,然后句子长度不够的append 1\n",
    "# 输入 [([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. [([1, 2, 3],3, 0), ([2, 3], 2, 1), ...]\n",
    "# 输出，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. 假设batch_size = 2\n",
    "# [([1, 2, 3], 3), ([2, 3, 1], 2),...] 第二个句子列表 append 1，但是实际长度为2\n",
    "\n",
    "# 3、【将句子进行分batch，并且将数字列表（句子）进行填充】\n",
    "def pad_sentence_plus(array_ls, array_sentiment, batch_size):  # 这里的array_ls是上面函数的输出，即sentences_tensor。\n",
    "    ans = 0  # j记录每个batch的第一条数据的下标\n",
    "    max = array_ls[0][1]  # max为每个batch的句子最大长度.\n",
    "    # 第一维为列表的第几个元素（即一个句子元组），第二维为一个元组的第一个元素（即一个元组里面的word_list），这里max初始化为第一个句子的长度。\n",
    "    for i in range(len(array_ls)):\n",
    "        # 需要考虑最后一个batch可能长度不够batch_size\n",
    "        if (i + 1) % batch_size == 0 or i == len(array_ls) - 1:\n",
    "            if array_ls[i][1] > max:\n",
    "                max = array_ls[i][1]\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                index = batch_size\n",
    "            else:\n",
    "                index = i - ans + 1\n",
    "            for j in range(index):\n",
    "                while len(array_ls[j + ans][0]) < max:\n",
    "                    array_ls[j + ans][0].append(1)\n",
    "                    array_sentiment[j + ans] += [0]\n",
    "            # 每一次填充完毕后，需要更新标记，并再次初始化最大值\n",
    "            ans = i + 1\n",
    "            if ans != len(array_ls):\n",
    "                max = array_ls[ans][1]\n",
    "        else:\n",
    "            if array_ls[i][1] > max:  # 取句子的最大长度\n",
    "                max = array_ls[i][1]\n",
    "    return array_ls, array_sentiment\n",
    "\n",
    "\n",
    "# 输入，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# [([1, 2, 3],3), ([2, 3, 1], 2),...]\n",
    "# shuffle表示是否将每个batch打乱\n",
    "# batch_first if false 表示返回的张量形状为(sentence_len,batch_size)\n",
    "# if true 表示返回的张量形状为(batch_size,sentence_len)\n",
    "# 这里默认采用batch_first = False,主要是为了适应nn.Embedding层的输入形状\n",
    "\n",
    "# 4、【获得embedding的一个个batch（调整batch的的形状）的数据迭代器】\n",
    "def iterator(array_ls, array_sentiment, batch_size, shuffle=True, batch_first=False):\n",
    "    sentences_index_label_sentiment = []  # 存放填充后的语句列表【句子的indx表示】\n",
    "    tmp_sen = []\n",
    "    tmp_label = []\n",
    "    tmp_sentiment = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tmp_sen.append(sentence[0])  # 存放一个batch的【数据】\n",
    "        tmp_label.append(sentence[2])  # 存放一个batch的【标签】\n",
    "        tmp_sentiment.append(array_sentiment[i])\n",
    "        if (i + 1) % batch_size == 0:  #\n",
    "            sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "            tmp_sen = []  # 清空数据\n",
    "            tmp_label = []  # 清空标签\n",
    "            tmp_sentiment = []\n",
    "\n",
    "    # 最后几个样本可能不够一个batch,需要额外判断\n",
    "    if len(tmp_sen) != 0:\n",
    "        # # 补齐\n",
    "        # for _ in range(batch_size - len(tmp_sen)):\n",
    "        #     tmp_sen.append(sentence[0])\n",
    "        #     tmp_label.append(sentence[2])\n",
    "        #     tmp_sentiment.append(array_sentiment[1])\n",
    "        sentences_index_label_sentiment.append((tmp_sen, tmp_label, tmp_sentiment))\n",
    "        # # OR去掉\n",
    "        # break\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(sentences_index_label_sentiment)  # 打乱列表中各个batch的顺序\n",
    "    res = []\n",
    "    # 2D张量转置\n",
    "    # 2D张量转置\n",
    "    if batch_first == False:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]),\n",
    "                        torch.FloatTensor(batch[2]).t()))  # 函数t()求矩阵的转置\n",
    "    else:\n",
    "        for batch in sentences_index_label_sentiment:\n",
    "            res.append((torch.LongTensor(batch[0]).t(), torch.FloatTensor(batch[1]), torch.FloatTensor(batch[2]).t()))\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "source": [
    "import torch\n",
    "import random\n",
    "from nltk.corpus import wordnet, sentiwordnet\n",
    "# 下载 SentiWordNet 数据\n",
    "# nltk.download('sentiwordnet')\n",
    "# 加载 SentiWordNet 数据\n",
    "swn_file_path = nltk.data.find('corpora/sentiwordnet/SentiWordNet_3.0.0.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "source": [
    "# =====================================================================================\n",
    "# ================================= sentiwordnet ======================================\n",
    "# =====================================================================================\n",
    "\n",
    "\n",
    "def get_sentiment_score(word):\n",
    "    \"\"\"获取WordNet情感分数\"\"\"\n",
    "\n",
    "    synsets = list(wordnet.synsets(word))\n",
    "    if synsets:\n",
    "        sentiment_scores = [\n",
    "            sentiwordnet.senti_synset(s.name()).pos_score() - sentiwordnet.senti_synset(s.name()).neg_score()\n",
    "            for s in synsets\n",
    "        ]\n",
    "        avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "        return avg_sentiment_score\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def get_word_sentiment_scores(tokenized_sentence):\n",
    "    \"\"\"计算已分词句子中每个单词的情感极性分数\"\"\"\n",
    "    # 处理否定词\n",
    "    word_scores = [get_sentiment_score(word) for word in tokenized_sentence]\n",
    "    return word_scores\n",
    "\n",
    "\n",
    "def get_batch_word_sentiment_scores(batch_tokenized_sentences):\n",
    "    \"\"\"处理一批已分词的文本并返回其中每个单词的情感极性分数\"\"\"\n",
    "    batch_word_scores = []\n",
    "    for sentence, _, _ in batch_tokenized_sentences:\n",
    "        batch_word_scores.append(get_word_sentiment_scores(sentence))\n",
    "    return batch_word_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED) # 为CPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.cuda.manual_seed(SEED) # 为GPU设置随机种子，生成随机数，使得每次运行程序时，生成的随机数相同\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_NUMBER}' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "source": [
    "def get_iterator(path, batch_size, word_to_index):\n",
    "    # ['another', 'thing', 'i', 'don', \"'t\", 'like', 'is', 'that', 'some', 'of', 'the',\n",
    "    # 'features', 'take', '1-2', 'seconds', 'to', 'load', 'up', '.']\n",
    "    sentences = get_sentences(path)\n",
    "    # ([2, 3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
    "    # 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 19, 0.0)\n",
    "    # 将单词变成indx\n",
    "    sentences_indx = lookup_table(sentences, word_to_index)\n",
    "\n",
    "    # get sentiment polarity by 【AFINN】\n",
    "    # afinn = Afinn()\n",
    "    # sentiment_sentences_AFINN = get_batch_word_sentiment_scores_AFINN(sentences,afinn)\n",
    "    # sentiment_sentences_AFINN = calculate_sentiment_for_phrases(sentences, afinn, n=3)\n",
    "\n",
    "    # get sentiment polarity by 【sentiwordnet】\n",
    "    sentiment_sentences_sentiwordnet = get_batch_word_sentiment_scores(sentences)\n",
    "    sentences_padded, sentiment_sentences_padded = pad_sentence_plus(sentences_indx, sentiment_sentences_sentiwordnet,\n",
    "                                                                     batch_size)\n",
    "\n",
    "    # 把分成一个个batch，然后每个list之内有数据和标签\n",
    "    Iterator_with_sentiments = iterator(sentences_padded, sentiment_sentences_padded, batch_size)\n",
    "\n",
    "    return Iterator_with_sentiments\n",
    "\n",
    "# 2、生成对应任务的句子的词向量\n",
    "train_iterator = get_iterator(train_path, BATCH_SIZE, word_to_index)\n",
    "test_iterator = get_iterator(test_path, BATCH_SIZE, word_to_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "source": [
    "class projection_Euler(nn.Module):\n",
    "    def __init__(self, Embedding, num_layers):\n",
    "        super(projection_Euler, self).__init__()  # projection是子类名\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        amplitude = inputs[0]\n",
    "        phase = inputs[1]\n",
    "        # ================ 变换形状和归一化处理 ================\n",
    "        # -----\n",
    "        amplitude_permute = amplitude.permute(1, 0, 2)\n",
    "        amplitude_norm = F.normalize(amplitude_permute, 2, 2)\n",
    "        # ----\n",
    "        phase_permute = phase.permute(1, 0, 2)\n",
    "\n",
    "        # ================ 计算Am和Ph分别计算实部和虚部 ================\n",
    "        # --- 求欧拉展开式\n",
    "        real_part = amplitude_norm * torch.cos(phase_permute)\n",
    "        imag_part = amplitude_norm * torch.sin(phase_permute)\n",
    "\n",
    "        return [real_part, imag_part]\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixMean(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixMean, self).__init__()\n",
    "        self.mapping_Diagonal_q = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.mapping_Diagonal_k = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.embed_size = Embedding_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        Diagonal1 = inputs[0]\n",
    "        Diagonal2 = inputs[1]\n",
    "        Matrix = inputs[2]\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attention_scores = Diagonal1 * Diagonal2\n",
    "        # 迹求和\n",
    "        attention_weight = torch.sum(attention_scores, dim=-1)\n",
    "\n",
    "        # 归一化\n",
    "        attention = F.softmax(attention_weight, dim=-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # 加权求和\n",
    "        Matrix_output = torch.sum(Matrix * attention, dim=1)\n",
    "\n",
    "        return Matrix_output\n",
    "\n",
    "\n",
    "class projection_CalculateMatrixQ(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_CalculateMatrixQ, self).__init__()\n",
    "        self.Q_Linear_real1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag1 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.Q_Linear_real2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "        self.Q_Linear_imag2 = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "        self.AttentionReal = projection_CalculateMatrixMean(Embedding_dim)\n",
    "        self.AttentionImag = projection_CalculateMatrixMean(Embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_real = inputs[0]  # 实部\n",
    "        input_imag = inputs[1]  # 虚部\n",
    "        # ==================== 1 计算对角线和密度矩阵 ====================\n",
    "\n",
    "        # ======= 1.1 计算对角线 =======\n",
    "        input_real1 = self.Q_Linear_real1(input_real)\n",
    "        input_imag1 = self.Q_Linear_imag1(input_imag)\n",
    "\n",
    "        # 获得[实部/虚部]对角线向量\n",
    "        diagonal_real1 = input_real1 * input_real1 - input_imag1 * input_imag1\n",
    "        diagonal_imag1 = input_real1 * input_imag1 + input_imag1 * input_real1\n",
    "\n",
    "        # ======= 1.2 计算密度矩阵 =======\n",
    "        input_real2 = self.Q_Linear_real2(input_real)\n",
    "        input_imag2 = self.Q_Linear_imag2(input_imag)\n",
    "\n",
    "        diagonal_real2 = input_real2 * input_real2 - input_imag2 * input_imag2\n",
    "        diagonal_imag2 = input_real2 * input_imag2 + input_imag2 * input_real2\n",
    "\n",
    "        real_part_expand = torch.unsqueeze(input_real2, dim=3)\n",
    "        imag_part_expand = torch.unsqueeze(input_imag2, dim=3)\n",
    "\n",
    "        real_part_expand_transpose = real_part_expand.permute(0, 1, 3, 2)\n",
    "        imag_part_expand_transpose = imag_part_expand.permute(0, 1, 3, 2)\n",
    "\n",
    "        # real 密度矩阵\n",
    "        Matrix_real = torch.matmul(real_part_expand, real_part_expand_transpose) - torch.matmul(imag_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "        # imag 密度矩阵\n",
    "        Matrix_imag = torch.matmul(imag_part_expand, real_part_expand_transpose) + torch.matmul(real_part_expand,\n",
    "                                                                                                imag_part_expand_transpose)\n",
    "\n",
    "        Matrix_Real_output = self.AttentionReal([diagonal_real1, diagonal_real2, Matrix_real])\n",
    "        Matrix_imag_output = self.AttentionImag([diagonal_imag1, diagonal_imag2, Matrix_imag])\n",
    "\n",
    "        return [Matrix_Real_output, Matrix_imag_output]\n",
    "\n",
    "\n",
    "class projection_Measurement(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection_Measurement, self).__init__()  # projection是子类名\n",
    "        # 使得tensor是正交的,正交初始化,projector是子类projection的新属性。\n",
    "        self.projector = nn.init.orthogonal_(Parameter(torch.Tensor(2, Embedding_dim, 1)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        v_real_avg = inputs[0]\n",
    "        v_imag_avg = inputs[1]\n",
    "\n",
    "        p_real = self.projector[0]\n",
    "        p_imag = self.projector[1]\n",
    "\n",
    "        p_real_norm = p_real / torch.norm(p_real, dim=0)\n",
    "        p_imag_norm = p_imag / torch.norm(p_imag, dim=0)\n",
    "\n",
    "        p_real_mat = torch.matmul(p_real_norm, p_real_norm.permute(1, 0))\n",
    "        p_imag_mat = torch.matmul(p_imag_norm, p_imag_norm.permute(1, 0))\n",
    "\n",
    "        Pv_real = torch.matmul(v_real_avg, p_real_mat) - torch.matmul(v_imag_avg, p_imag_mat)\n",
    "        Pv_imag = torch.matmul(v_real_avg, p_imag_mat) + torch.matmul(v_imag_avg, p_real_mat)\n",
    "\n",
    "        Pv_real_plus = torch.unsqueeze(Pv_real, dim=1)\n",
    "        Pv_imag_plus = torch.unsqueeze(Pv_imag, dim=1)\n",
    "\n",
    "        return [Pv_real_plus, Pv_imag_plus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "source": [
    "# 映射成三个不同的向量的自注意力机制\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.mapping_query = nn.init.uniform_(\n",
    "            Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))  # 从均匀分布U(a, b)中生成值（随机生成），填充输入的张量或变量\n",
    "        self.mapping_key = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "        self.mapping_value = nn.init.uniform_(Parameter(torch.Tensor(Embedding_dim, Embedding_dim)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        query = torch.matmul(inputs.permute(1, 0, 2), self.mapping_query)  # permute函数为换维函数，参数为(0，1，2)\n",
    "        key = torch.matmul(inputs.permute(1, 0, 2), self.mapping_key)  # matmul是tensor矩阵乘法\n",
    "        value = torch.matmul(inputs.permute(1, 0, 2), self.mapping_value)\n",
    "\n",
    "        query = query / torch.norm(query, dim=2).reshape(query.shape[0], query.shape[1],\n",
    "                                                         1)  # query.shape[0]=16,query.shape[1]=sentence_length.\n",
    "        key = key / torch.norm(key, dim=2).reshape(key.shape[0], key.shape[1],\n",
    "                                                   1)  # shape of tensor:[16,sentence_length,1]\n",
    "\n",
    "        value = value / torch.norm(value, dim=2).reshape(value.shape[0], value.shape[1], 1)  # query,key,value的形状是一样的\n",
    "\n",
    "        xx = torch.matmul(query, torch.transpose(key, 1, 2)) / np.sqrt(inputs.shape[1])\n",
    "        xx1 = torch.softmax(xx, dim=2)\n",
    "        out = torch.matmul(xx1, value)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "source": [
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, Embedding_dim, num_layers):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "\n",
    "        # GRU layer\n",
    "        self.rnn = nn.GRU(Embedding_dim, Embedding_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "        # self.scale_layer = nn.Linear(2 * Embedding_dim, Embedding_dim)\n",
    "\n",
    "        # Linear layer for skip connection\n",
    "        self.middle_layer = nn.Linear(Embedding_dim, Embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through GRU layer\n",
    "        output, _ = self.rnn(x)\n",
    "\n",
    "        # Skip connection: Add output of middle layer to GRU layer output\n",
    "        output = output + self.middle_layer(x)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "source": [
    "# -------------------- 01 Model_best_copy8672_cnn_maxpooling ------------------------\n",
    "print(\"Model_best_copy8672_cnn_maxpooling\")\n",
    "class run_complex_network(nn.Module):\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, output_dim, num_layers, pad_idx, VOCAB_SIZE, batch):\n",
    "        super().__init__()\n",
    "\n",
    "        # ================ 1、词嵌入模块 ================\n",
    "        # ------ 实部，预训练模型 -------\n",
    "        self.amplitude_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "\n",
    "        # self.amplitude_embedding2 = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ------ 虚部，传统模型 -------\n",
    "        self.phase_embedding = nn.Embedding.from_pretrained(weight_matrix, padding_idx=pad_idx, freeze=False)\n",
    "        # self.phase_embedding = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "        # self.phase_embedding_sentiment = nn.Embedding(VOCAB_SIZE, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ================ 2、GRU模块 信息提取 ================\n",
    "        # 输出张量的维度(L:sequence_length,N:batch_size,hidden_size)\n",
    "        self.gru1 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "        self.gru2 = GRUWithSkipConnection(embedding_dim, num_layers)\n",
    "\n",
    "        self.self_attention1 = self_attention(Embedding_dim=embedding_dim)\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "\n",
    "        self.projection_Euler = projection_Euler(Embedding=embedding_dim, num_layers=num_layers)\n",
    "        self.projection_CalculateMatrixattension = projection_CalculateMatrixQ(Embedding_dim=embedding_dim)\n",
    "        self.projection_measurement = projection_Measurement(embedding_dim)\n",
    "\n",
    "        # ================ 4、卷积 ================\n",
    "        self.Conv2dOne = nn.Conv2d(1, 1, 3)  # 实部\n",
    "        self.Conv2dTwo = nn.Conv2d(1, 1, 3)  # 虚部\n",
    "\n",
    "        # ================ 5、池化 ================\n",
    "\n",
    "        self.MaxPool1 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 实部\n",
    "        self.MaxPool2 = nn.MaxPool2d((embedding_dim - 2, 1), 1)  # 虚部\n",
    "\n",
    "        # ================ 6、全连接层 ================\n",
    "        self.fc1 = nn.Linear(2 * (embedding_dim - 2), 10)\n",
    "        self.fc2 = nn.Linear(10, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # ================ 1、文本嵌入到向量 ================\n",
    "        amplitude_is_WordEmbedding = self.amplitude_embedding(text)\n",
    "\n",
    "        phase_is_sentiment = self.phase_embedding(text)\n",
    "\n",
    "        # ================ 2、实部进行GRU处理 ================\n",
    "        amplitude_plus = self.gru1(amplitude_is_WordEmbedding)\n",
    "        amplitude_plus2 = self.self_attention1(amplitude_plus)\n",
    "        amplitude_plus2 = amplitude_plus2.permute(1, 0, 2)\n",
    "\n",
    "        phase_plus = self.gru2(phase_is_sentiment)\n",
    "\n",
    "        # ================ 3、计算密度矩阵 ================\n",
    "        embedded = [amplitude_plus2, phase_plus]\n",
    "\n",
    "        Euler_realAimag = self.projection_Euler(embedded)\n",
    "        Matrix_realAimag = self.projection_CalculateMatrixattension(Euler_realAimag)\n",
    "        Project_realAimag = self.projection_measurement(Matrix_realAimag)\n",
    "\n",
    "        MatrixReal = Project_realAimag[0]\n",
    "        MatrixImag = Project_realAimag[1]\n",
    "\n",
    "        # ================ 4、卷积操作 ================\n",
    "        # 5.1 --对于【实部】进行卷积\n",
    "        Conv_real = self.Conv2dOne(MatrixReal)\n",
    "        # 5.2 --对于【虚部】进行卷积\n",
    "        Conv_imag = self.Conv2dTwo(MatrixImag)\n",
    "\n",
    "        # ================ 5、池化操作 ================\n",
    "        # 6.1 --对于【实部】进行池化\n",
    "        Max_real = self.MaxPool1(torch.sigmoid(Conv_real))\n",
    "        # 6.2 --对于【虚部】进行池化\n",
    "        Max_imag = self.MaxPool2(torch.sigmoid(Conv_imag))\n",
    "\n",
    "        # ================ 6、全连接 ================\n",
    "        fc1 = self.fc1(torch.cat((Max_real, Max_imag), dim=3))\n",
    "        fc2 = torch.sigmoid(self.fc2(torch.sigmoid(fc1)))\n",
    "\n",
    "        return fc2  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_best_copy8672_cnn_maxpooling\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def multiply(input1, input2):\n",
    "    return torch.sum(torch.mul(input1, input2)).item()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    # print('batch: 模型预测值', rounded_preds,'真实标签', y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def calculation_recall(preds, y):\n",
    "    pre = preds.detach()\n",
    "    pre_y = y.detach()\n",
    "    rounded_preds = torch.round(pre)\n",
    "    \n",
    "    TP = (torch.from_numpy(rounded_preds.cpu().numpy().astype(int)) & torch.from_numpy(pre_y.cpu().numpy().astype(int))).sum()\n",
    "    recall = TP / (y.cpu().sum())\n",
    "    return recall\n",
    "\n",
    "def f1_score_avg(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    f1_score(y, rounded_preds, 'macro')\n",
    "\n",
    "# 定义训练流程\n",
    "# 把训练数据投入模型进行训练\n",
    "def train(model, iterator, optimizer, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        text = batch[0].cuda(CUDA_NUMBER)\n",
    "        label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text).squeeze(1)\n",
    "        # print(predictions)\n",
    "        loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "        acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "        recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "        label_score = label.tolist()\n",
    "\n",
    "        y_true = label_score.copy()\n",
    "\n",
    "        predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "\n",
    "        y_pred = predictions_score.copy()\n",
    "\n",
    "        F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        epoch_recall += recall.item()\n",
    "\n",
    "        epoch_f1_score += F1Score.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "# 评价\n",
    "def evaluate(model, iterator, criterion, CUDA_NUMBER):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_recall = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch[0].cuda(CUDA_NUMBER)\n",
    "            label = batch[1].cuda(CUDA_NUMBER)\n",
    "\n",
    "            predictions = model(text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions.reshape(len(label)), label)\n",
    "\n",
    "            acc = binary_accuracy(predictions.reshape(len(label)), label)\n",
    "\n",
    "            recall = calculation_recall(predictions.reshape(len(label)), label)\n",
    "\n",
    "            label_score = label.tolist()\n",
    "            y_true = label_score.copy()\n",
    "            predictions_score = torch.round(predictions.reshape(len(label))).tolist()\n",
    "            y_pred = predictions_score.copy()\n",
    "            F1Score = f1_score(y_true, y_pred)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1_score += F1Score.item()\n",
    "            epoch_recall += recall.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1_score / len(\n",
    "        iterator), epoch_recall / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "num_layers = 2\n",
    "print(\"num_layers\",num_layers)\n",
    "# vocab_size = 19192, embedding_dim = 100, hidden_dim = 3, output_dim = 1, dropout = 0.5, pad_idx = 1\n",
    "PAD_IDX = 1\n",
    "print('pad_idx', PAD_IDX)\n",
    "\n",
    "model = run_complex_network(word_vector_weight_matrix,\n",
    "                            EMBEDDING_DIM,\n",
    "                            HIDDEN_DIM,\n",
    "                            OUTPUT_DIM,\n",
    "                            num_layers,\n",
    "                            PAD_IDX,\n",
    "                            VOCAB_SIZE,\n",
    "                            BATCH_SIZE)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())#优化器使用的是Adam\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "best_test_acc = float('-inf')\n",
    "print(0.5 > best_test_acc)\n",
    "\n",
    "best_acc = -1\n",
    "best_f1_score = -1\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "f1_score_test = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc, train_f1_score, train_recall \\\n",
    "        = train(model, train_iterator, optimizer, criterion, CUDA_NUMBER)\n",
    "\n",
    "    test_loss, test_acc, test_f1_score, test_recall \\\n",
    "        = evaluate(model, test_iterator, criterion, CUDA_NUMBER)\n",
    "\n",
    "    acc_train.append(train_acc)\n",
    "    acc_test.append(test_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    loss_test.append(test_loss)\n",
    "    f1_score_test.append(test_f1_score)\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'SST_GRU_Conv_model.pt')\n",
    "\n",
    "    # print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    # print(\n",
    "    #     f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}% | Train f1_score: {train_f1_score * 100:.2f}% | Train recall: {train_recall * 100:.2f}%')\n",
    "    # print(\n",
    "    #     f'\\t test. Loss: {test_loss:.3f} |  test. Acc: {test_acc * 100:.2f}% | Test f1_score: {test_f1_score * 100:.2f}%| best_acc: {best_acc * 100:.2f}% | Test recall: {test_recall * 100:.2f}%')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t test  Loss: {test_loss:.3f} | test  Acc: {test_acc * 100:.2f}%')\n",
    "    print(f'\\t best  test acc: {best_test_acc * 100:.2f}%')\n",
    "\n",
    "with open('./train/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_train:\n",
    "        f.write(str(i))\n",
    "    f.close()\n",
    "\n",
    "with open('./test/loss/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in loss_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/F1_Score/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in f1_score_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/acc/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in acc_test:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "with open('./test/recall/' + DATA_SET + '.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in test_path_ls:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "#\n",
    "colors1 = '#00CED1' \n",
    "colors2 = '#DC143C'\n",
    "colors3 = '#FFA500'\n",
    "colors4 = '#0000aFF'\n",
    "colors5 = '#FF0000'\n",
    "\n",
    "# 画初始点\n",
    "plt.figure(1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(DATA_SET + ' Loss')\n",
    "plt.plot(range(N_EPOCHS), acc_train[0:N_EPOCHS], 'r-o', label='train_loss')\n",
    "plt.plot(range(N_EPOCHS), acc_test[0:N_EPOCHS], 'c-*', label='test_loss')\n",
    "plt.xlim(-0.01, N_EPOCHS)\n",
    "plt.ylim(-0.01, 1)\n",
    "# plt.plot(range(2), sp, c=colors3)\n",
    "plt.legend()\n",
    "plt.savefig('pic' + DATA_SET + '.pdf', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num_layers 2\n",
      "pad_idx 1\n",
      "The model has 3,130,001 trainable parameters\n",
      "True\n",
      "Epoch: 01 | Epoch Time: 1m 24s\n",
      "\t Train Loss: 0.458 | Train Acc: 77.86%\n",
      "\t test  Loss: 0.437 | test  Acc: 81.32%\n",
      "\t best  test acc: 81.32%\n",
      "Epoch: 02 | Epoch Time: 1m 25s\n",
      "\t Train Loss: 0.246 | Train Acc: 90.69%\n",
      "\t test  Loss: 0.415 | test  Acc: 82.91%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 03 | Epoch Time: 1m 23s\n",
      "\t Train Loss: 0.179 | Train Acc: 93.41%\n",
      "\t test  Loss: 0.422 | test  Acc: 82.70%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 04 | Epoch Time: 1m 23s\n",
      "\t Train Loss: 0.143 | Train Acc: 94.88%\n",
      "\t test  Loss: 0.440 | test  Acc: 82.60%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 05 | Epoch Time: 1m 22s\n",
      "\t Train Loss: 0.119 | Train Acc: 95.63%\n",
      "\t test  Loss: 0.506 | test  Acc: 81.98%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 06 | Epoch Time: 1m 22s\n",
      "\t Train Loss: 0.098 | Train Acc: 96.37%\n",
      "\t test  Loss: 0.508 | test  Acc: 82.53%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 07 | Epoch Time: 1m 20s\n",
      "\t Train Loss: 0.085 | Train Acc: 96.82%\n",
      "\t test  Loss: 0.565 | test  Acc: 82.60%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 08 | Epoch Time: 1m 22s\n",
      "\t Train Loss: 0.077 | Train Acc: 97.09%\n",
      "\t test  Loss: 0.533 | test  Acc: 82.70%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 09 | Epoch Time: 1m 23s\n",
      "\t Train Loss: 0.064 | Train Acc: 97.55%\n",
      "\t test  Loss: 0.614 | test  Acc: 81.50%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 10 | Epoch Time: 1m 21s\n",
      "\t Train Loss: 0.062 | Train Acc: 97.63%\n",
      "\t test  Loss: 0.701 | test  Acc: 80.77%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 11 | Epoch Time: 1m 20s\n",
      "\t Train Loss: 0.060 | Train Acc: 97.62%\n",
      "\t test  Loss: 0.657 | test  Acc: 82.21%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 12 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 0.050 | Train Acc: 98.04%\n",
      "\t test  Loss: 0.627 | test  Acc: 82.10%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 13 | Epoch Time: 1m 9s\n",
      "\t Train Loss: 0.046 | Train Acc: 98.19%\n",
      "\t test  Loss: 0.821 | test  Acc: 80.73%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 14 | Epoch Time: 1m 10s\n",
      "\t Train Loss: 0.043 | Train Acc: 98.26%\n",
      "\t test  Loss: 0.743 | test  Acc: 80.14%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 15 | Epoch Time: 1m 9s\n",
      "\t Train Loss: 0.039 | Train Acc: 98.36%\n",
      "\t test  Loss: 0.784 | test  Acc: 80.87%\n",
      "\t best  test acc: 82.91%\n",
      "Epoch: 16 | Epoch Time: 1m 9s\n",
      "\t Train Loss: 0.035 | Train Acc: 98.56%\n",
      "\t test  Loss: 0.792 | test  Acc: 80.69%\n",
      "\t best  test acc: 82.91%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}